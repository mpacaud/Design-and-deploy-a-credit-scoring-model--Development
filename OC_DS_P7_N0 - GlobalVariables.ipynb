{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 7 - Implementation of a scoring model\n",
    "# Notebook - Global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Projet-7---Implementation-of-a-scoring-model\" data-toc-modified-id=\"Projet-7---Implementation-of-a-scoring-model-1\">Projet 7 - Implementation of a scoring model</a></span></li><li><span><a href=\"#Notebook---Global-variables\" data-toc-modified-id=\"Notebook---Global-variables-2\">Notebook - Global variables</a></span></li><li><span><a href=\"#I)-Importation-of-required-libraries\" data-toc-modified-id=\"I)-Importation-of-required-libraries-3\">I) Importation of required libraries</a></span></li><li><span><a href=\"#II)-Global-variables\" data-toc-modified-id=\"II)-Global-variables-4\">II) Global variables</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Importation of required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Librairies and tools to import ###\\n\\n# File system management.\\nimport os.path\\n\\n# Data manipulations.\\nimport numpy as np\\nimport pandas as pd\\n\\n# Time measurment and datetime management.\\nimport datetime as dt\\n#import time\\nfrom time import time\\n\\n# Python random sampling.\\nfrom random import sample as py_rd_sp\\n\\n# Warnings suppression.\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Data visualizations.\\nfrom pprint import pprint\\n#from pandas.plotting import scatter_matrix\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Warnings management.\\nimport warnings\\nwarnings.simplefilter(action='ignore', category=FutureWarning)\\n\\n# Saving full data.\\nfrom numpy import set_printoptions\\n\\n\\n### Set default figure parameters for the whole notebook ###\\n\\n# Default parameters for matplotlib's figures.\\nplt.rcParams['figure.figsize'] = [6,6]\\nplt.rcParams['figure.dpi'] = 200\\n#mpl.rcParams['axes.prop_cycle'] = cycler(color=['b', 'r', 'g'])\\nplt.rcParams['axes.spines.right'] = False\\nplt.rcParams['axes.spines.top'] = False\\nplt.rcParams['xtick.bottom'] = True\\nplt.rcParams['ytick.left'] = True\\n\\n# Default parameters of seaborn's figures.\\nsns.set_style('white') # NB: Needs to be above sns.set_theme to properly attend custom_params.\\ncustom_params = {'axes.spines.right': False, 'axes.spines.top': False}\\nsns.set_theme(style='ticks', palette='deep', rc=custom_params)\\n\\n\\n# Global file paths.\\n#EXPORTS_DIR_PATH = 'Exports'\\nEXPORTS_MODELS_DIR_PATH = r'Exports\\\\Models\\\\Tried'\\nIMPORTS_DIR_PATH = r'Exports\\\\Preprocessed_data'\\n\\nCSV_MODELS_FILE = 'models_info.csv'\\nPKL_MODELS_FILE = 'models_info.pkl'\\n#JSON_MODELS_FILE = 'models_info.json'\\n#DATASETS_DIR_PATH = r'D:\\x00Partage\\\\MP-P2PNet\\\\MP-Sync\\\\MP-Sync_Pro\\\\Info\\\\OC_DS\\\\Projet 7\\\\Datasets' #os.path.join('D:', '0Partage', 'MP-P2PNet', 'MP-Sync', 'MP-Sync_Pro', 'Info', 'OC_DS', 'Projet 7', 'Datasets')\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### File management ###\n",
    "\n",
    "# Files' path.\n",
    "import os.path\n",
    "\n",
    "# Save and load files.\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "\n",
    "### Data manipulations ###\n",
    "\n",
    "import numpy as np\n",
    "from numpy import set_printoptions # Saving full data when exporting to csv format.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "### Date & time ###\n",
    "\n",
    "# Time measurment and datetime management\n",
    "import datetime as dt\n",
    "from time import time\n",
    "\n",
    "\n",
    "### Warnings removal ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "### Data visualizations ###\n",
    "\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "### Additional common libraries ###\n",
    "\n",
    "from numpy import argmax, argmin\n",
    "import math\n",
    "from random import sample as py_rd_sp # Python random sampling.\n",
    "\n",
    "# Those allow to transform the shap values from their logodd format to odd.\n",
    "import copy\n",
    "from scipy.special import expit # Opposed of logit.\n",
    "\n",
    "\n",
    "### sklearn tools & libraries ###\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, precision_recall_curve, fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import make_scorer # Allow to make a sklearn custom scorer (For the custom job score).\n",
    "\n",
    "\n",
    "### Imbalanced data management ###\n",
    "\n",
    "from imblearn.pipeline import Pipeline # NB: imbalearn.pipeline.Pipeline allows to properly deal the SMOTE on the train set and avoid the validation/test sets.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTENC # NB: SMOTENC can manage categorial features while SMOTE cannot.\n",
    "\n",
    "\n",
    "### Bayesian hyperparmaters tuning ###\n",
    "\n",
    "# Hyperopt modules.\n",
    "from hyperopt import STATUS_OK # Check if the objective function returned a valid value (Mandatory).\n",
    "\n",
    "# Methods for the domain space, algorithm optimization, save the trials history, bayesian optimization.\n",
    "from hyperopt import hp, tpe, Trials, fmin, pyll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Initialize the default cross validation method to use.\\nSKF_5 = StratifiedKFold(5, shuffle=True, random_state=0)\\n\\n# True: Allows hyperprameter tuning, False: Get the results stored from the last hyperparameters tuning.\\nHT = True\\n\\n# For imbalanced data use weight or data sampling.\\nIMB_PROCESS = \\'Weight\\' #\\'Resp\\'\\n\\n# Global common scaler to use.\\nSCALER = MinMaxScaler()\\n\\n# Update the csv file containing the training information and scores of the model or not (True = update).\\nGET_CSV_FILE = True\\n\\n# Set and initialize the main scorer used for the models comparisons.\\nMAIN_SCORER_TRAIN_LABEL = \\'Job_score_train\\'\\nMAIN_SCORER_TEST_LABEL = \\'Job_score_test\\'\\nMAIN_SCORER_VAL = 0\\n\\n# Load/create and initialize the dataframe in which store all relevant models\\' information (best hyperparameters, scores...).\\n# NB: In case of the creation of the file data=np.full((1,len(l_COL_LABELS)), None) to force dtypes as objects\\n#     until one of the next added entries (rows) are full then, it will be removed. Otherwise, the np.nan values which will appear\\n#     within the first row will convert their columns\\' dtypes to float64 and prevent their replacement\\n#     by objects such as np.array.\\nl_COL_LABELS = [\\'Model_labels\\', \\'Models\\',\\n                \\'yhat_train\\', \\'yhat_test\\',\\n                \\'Best_proba_threshold_train\\', \\'Best_proba_threshold_test\\',\\n                \\'Job_score_train\\', \\'Job_score_test\\', \\n                \\'AUROC_scores_train\\', \\'AUROC_scores_test\\',\\n                \\'F-bêta_score_train\\', \\'F-bêta_score_test\\',\\n                \\'Process_time_train (s)\\', \\'Process_time_test (s)\\'\\n               ]\\n\\n#l_COL_LABELS = [\\'Model_labels\\', \\'Models\\',\\n#                \\'X_train_shape\\', \\'X_test_shape\\',\\n#                \\'yhat_train\\', \\'yhat_test\\',\\n#                \\'Best_proba_threshold_train\\', \\'Best_proba_threshold_test\\',\\n#                \\'Job_score_train\\', \\'Job_score_test\\', \\n#                \\'AUROC_scores_train\\', \\'AUROC_scores_test\\',\\n#                \\'F-bêta_score_train\\', \\'F-bêta_score_test\\',\\n#                \\'Process_time_train (s)\\', \\'Process_time_test (s)\\'\\n#               ]\\n\\nif GET_CSV_FILE:\\n    try:\\n        df_MODELS = pd.read_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))#.set_index(\\'Model_labels\\')\\n\\n    except:\\n        print(\"No csv models informations were found. A new one is created...\")\\n        df_MODELS = pd.DataFrame(data=np.full((1,len(l_COL_LABELS)), None), columns=l_COL_LABELS).set_index(\\'Model_labels\\')\\n        df_MODELS.to_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))\\n        print(\\'Done !\\')\\n    \\nelse:\\n    print(\"Creation of a new csv file to store models informations...\")\\n    df_MODELS = pd.DataFrame(data=np.full((1,len(l_COL_LABELS)), None), columns=l_COL_LABELS).set_index(\\'Model_labels\\')\\n    df_MODELS.to_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))\\n    print(\\'Done !\\')  \\n\\ndisplay(df_MODELS.info())'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the default cross validation method to use.\n",
    "SKF_5 = StratifiedKFold(5, shuffle=True, random_state=0)\n",
    "\n",
    "# True: Allows hyperprameter tuning, False: Get the results stored from the last hyperparameters tuning.\n",
    "#HT = True\n",
    "\n",
    "# For imbalanced data use weight or data sampling.\n",
    "#IMB_PROCESS = 'Weight' #'Resp'\n",
    "\n",
    "# Global common scaler to use.\n",
    "#SCALER = MinMaxScaler()\n",
    "\n",
    "# Update the csv file containing the training information and scores of the model or not (True = update).\n",
    "#GET_CSV_FILE = True\n",
    "\n",
    "# Set and initialize the main scorer used for the models comparisons.\n",
    "MAIN_SCORER_TRAIN_LABEL = 'Job_score_train'\n",
    "MAIN_SCORER_TEST_LABEL = 'Job_score_test'\n",
    "MAIN_SCORER_VAL = 0\n",
    "\n",
    "# Load/create and initialize the dataframe in which store all relevant models' information (best hyperparameters, scores...).\n",
    "# NB: In case of the creation of the file data=np.full((1,len(l_COL_LABELS)), None) to force dtypes as objects\n",
    "#     until one of the next added entries (rows) are full then, it will be removed. Otherwise, the np.nan values which will appear\n",
    "#     within the first row will convert their columns' dtypes to float64 and prevent their replacement\n",
    "#     by objects such as np.array.\n",
    "l_COL_LABELS = ['Model_labels', 'Models',\n",
    "                'yhat_train', 'yhat_test',\n",
    "                'Best_proba_threshold_train', 'Best_proba_threshold_test',\n",
    "                'Job_score_train', 'Job_score_test', \n",
    "                'AUROC_scores_train', 'AUROC_scores_test',\n",
    "                'F-bêta_score_train', 'F-bêta_score_test',\n",
    "                'Process_time_train (s)', 'Process_time_test (s)'\n",
    "               ]\n",
    "\n",
    "#l_COL_LABELS = ['Model_labels', 'Models',\n",
    "#                'X_train_shape', 'X_test_shape',\n",
    "#                'yhat_train', 'yhat_test',\n",
    "#                'Best_proba_threshold_train', 'Best_proba_threshold_test',\n",
    "#                'Job_score_train', 'Job_score_test', \n",
    "#                'AUROC_scores_train', 'AUROC_scores_test',\n",
    "#                'F-bêta_score_train', 'F-bêta_score_test',\n",
    "#                'Process_time_train (s)', 'Process_time_test (s)'\n",
    "#               ]\n",
    "\n",
    "if GET_CSV_FILE:\n",
    "    try:\n",
    "        df_MODELS = pd.read_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))#.set_index('Model_labels')\n",
    "\n",
    "    except:\n",
    "        print(\"No csv models informations were found. A new one is created...\")\n",
    "        df_MODELS = pd.DataFrame(data=np.full((1,len(l_COL_LABELS)), None), columns=l_COL_LABELS).set_index('Model_labels')\n",
    "        df_MODELS.to_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))\n",
    "        print('Done !')\n",
    "    \n",
    "else:\n",
    "    print(\"Creation of a new csv file to store models informations...\")\n",
    "    df_MODELS = pd.DataFrame(data=np.full((1,len(l_COL_LABELS)), None), columns=l_COL_LABELS).set_index('Model_labels')\n",
    "    df_MODELS.to_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))\n",
    "    print('Done !')  \n",
    "\n",
    "display(df_MODELS.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "544px",
    "width": "676px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "347.038px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
