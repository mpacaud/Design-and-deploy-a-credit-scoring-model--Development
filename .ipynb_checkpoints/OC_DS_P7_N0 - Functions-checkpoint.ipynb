{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 7 - Implementation of a scoring model\n",
    "# Notebook - Shared Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sources\n",
    "\n",
    "The webpage containing all data and descriptions: <a href=\"https://www.kaggle.com/c/home-credit-default-risk/data\" target=\"_blank\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__- TP:__ True positives correspond to customers which are classified as they would default the repayment of their loan and they would as expected.<br>\n",
    "__- FP:__ False positives correspond to customers which were guessed trustless to repay their loans whereas they would have to (Secondary case to avoid and minimize if possible).<br>\n",
    "__- FN:__ False negatives correspond to customers which were guessed trustful to repay their loans whereas they will not (Worst case to absolutly minimize).<br>\n",
    "__- TN:__ True negatives correspond to customers which are classified as they would not default the repayment of their loan and they don't as expected.<br>\n",
    "__- dt_sp:__ Data sampling.<br>\n",
    "__- wt:__ Weight.<br>\n",
    "__- opt:__ Optimal.<br>\n",
    "__- synth_sp:__ Synthetic sampling.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Projet-7---Implementation-of-a-scoring-model\" data-toc-modified-id=\"Projet-7---Implementation-of-a-scoring-model-1\">Projet 7 - Implementation of a scoring model</a></span></li><li><span><a href=\"#Notebook---Shared-Functions\" data-toc-modified-id=\"Notebook---Shared-Functions-2\">Notebook - Shared Functions</a></span></li><li><span><a href=\"#Data-sources\" data-toc-modified-id=\"Data-sources-3\">Data sources</a></span></li><li><span><a href=\"#Glossary\" data-toc-modified-id=\"Glossary-4\">Glossary</a></span></li><li><span><a href=\"#I)-Importation-of-the-dataset-into-a-pandas-dataframe\" data-toc-modified-id=\"I)-Importation-of-the-dataset-into-a-pandas-dataframe-5\">I) Importation of the dataset into a pandas dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#1)-Import-all-librairies-and-tools-required-to-realize-the-project-and-set-the-first-global-variables\" data-toc-modified-id=\"1)-Import-all-librairies-and-tools-required-to-realize-the-project-and-set-the-first-global-variables-5.1\">1) Import all librairies and tools required to realize the project and set the first global variables</a></span></li><li><span><a href=\"#1)-Importation-of-required-libraries\" data-toc-modified-id=\"1)-Importation-of-required-libraries-5.2\">1) Importation of required libraries</a></span></li><li><span><a href=\"#2)-Global-variables\" data-toc-modified-id=\"2)-Global-variables-5.3\">2) Global variables</a></span></li><li><span><a href=\"#1)-Basic-functions\" data-toc-modified-id=\"1)-Basic-functions-5.4\">1) Basic functions</a></span></li><li><span><a href=\"#2)-Importation-of-the-preprocessed-datasets\" data-toc-modified-id=\"2)-Importation-of-the-preprocessed-datasets-5.5\">2) Importation of the preprocessed datasets</a></span></li></ul></li><li><span><a href=\"#II)-Models\" data-toc-modified-id=\"II)-Models-6\">II) Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#3)-Functions\" data-toc-modified-id=\"3)-Functions-6.1\">3) Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#a)-Model-fitting-and-predictions\" data-toc-modified-id=\"a)-Model-fitting-and-predictions-6.1.1\">a) Model fitting and predictions</a></span></li><li><span><a href=\"#b)-Optimization-of-the-probability-threshold\" data-toc-modified-id=\"b)-Optimization-of-the-probability-threshold-6.1.2\">b) Optimization of the probability threshold</a></span></li><li><span><a href=\"#c)-AUROC\" data-toc-modified-id=\"c)-AUROC-6.1.3\">c) AUROC</a></span></li><li><span><a href=\"#d)-F-bêta-score\" data-toc-modified-id=\"d)-F-bêta-score-6.1.4\">d) F-bêta score</a></span></li><li><span><a href=\"#e)-Job-score\" data-toc-modified-id=\"e)-Job-score-6.1.5\">e) Job score</a></span></li><li><span><a href=\"#f)-Hyperparameter-tuning\" data-toc-modified-id=\"f)-Hyperparameter-tuning-6.1.6\">f) Hyperparameter tuning</a></span></li><li><span><a href=\"#g)-Model-peformance-evaluation\" data-toc-modified-id=\"g)-Model-peformance-evaluation-6.1.7\">g) Model peformance evaluation</a></span></li><li><span><a href=\"#h)-Table-to-store-all-models'-relevant-values-along-the-notebook\" data-toc-modified-id=\"h)-Table-to-store-all-models'-relevant-values-along-the-notebook-6.1.8\">h) Table to store all models' relevant values along the notebook</a></span></li></ul></li></ul></li><li><span><a href=\"#Interpretations\" data-toc-modified-id=\"Interpretations-7\">Interpretations</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#SHAP\" data-toc-modified-id=\"SHAP-7.0.1\">SHAP</a></span><ul class=\"toc-item\"><li><span><a href=\"#Library-importation\" data-toc-modified-id=\"Library-importation-7.0.1.1\">Library importation</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-7.0.1.2\">Functions</a></span></li><li><span><a href=\"#Shap-explanation\" data-toc-modified-id=\"Shap-explanation-7.0.1.3\">Shap explanation</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Importation of the dataset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import all librairies and tools required to realize the project and set the first global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Librairies and tools to import ###\\n\\n# File system management.\\nimport os.path\\n\\n# Data manipulations.\\nimport numpy as np\\nimport pandas as pd\\n\\n# Time measurment and datetime management.\\nimport datetime as dt\\n#import time\\nfrom time import time\\n\\n# Python random sampling.\\nfrom random import sample as py_rd_sp\\n\\n# Warnings suppression.\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# Data visualizations.\\nfrom pprint import pprint\\n#from pandas.plotting import scatter_matrix\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Warnings management.\\nimport warnings\\nwarnings.simplefilter(action='ignore', category=FutureWarning)\\n\\n# Saving full data.\\nfrom numpy import set_printoptions\\n\\n\\n### Set default figure parameters for the whole notebook ###\\n\\n# Default parameters for matplotlib's figures.\\nplt.rcParams['figure.figsize'] = [6,6]\\nplt.rcParams['figure.dpi'] = 200\\n#mpl.rcParams['axes.prop_cycle'] = cycler(color=['b', 'r', 'g'])\\nplt.rcParams['axes.spines.right'] = False\\nplt.rcParams['axes.spines.top'] = False\\nplt.rcParams['xtick.bottom'] = True\\nplt.rcParams['ytick.left'] = True\\n\\n# Default parameters of seaborn's figures.\\nsns.set_style('white') # NB: Needs to be above sns.set_theme to properly attend custom_params.\\ncustom_params = {'axes.spines.right': False, 'axes.spines.top': False}\\nsns.set_theme(style='ticks', palette='deep', rc=custom_params)\\n\\n\\n# Global file paths.\\n#EXPORTS_DIR_PATH = 'Exports'\\nEXPORTS_MODELS_DIR_PATH = r'Exports\\\\Models\\\\Tried'\\nIMPORTS_DIR_PATH = r'Exports\\\\Preprocessed_data'\\n\\nCSV_MODELS_FILE = 'models_info.csv'\\nPKL_MODELS_FILE = 'models_info.pkl'\\n#JSON_MODELS_FILE = 'models_info.json'\\n#DATASETS_DIR_PATH = r'D:\\x00Partage\\\\MP-P2PNet\\\\MP-Sync\\\\MP-Sync_Pro\\\\Info\\\\OC_DS\\\\Projet 7\\\\Datasets' #os.path.join('D:', '0Partage', 'MP-P2PNet', 'MP-Sync', 'MP-Sync_Pro', 'Info', 'OC_DS', 'Projet 7', 'Datasets')\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"### Librairies and tools to import ###\n",
    "\n",
    "# File system management.\n",
    "import os.path\n",
    "\n",
    "# Data manipulations.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Time measurment and datetime management.\n",
    "import datetime as dt\n",
    "#import time\n",
    "from time import time\n",
    "\n",
    "# Python random sampling.\n",
    "from random import sample as py_rd_sp\n",
    "\n",
    "# Warnings suppression.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data visualizations.\n",
    "from pprint import pprint\n",
    "#from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Warnings management.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Saving full data.\n",
    "from numpy import set_printoptions\n",
    "\n",
    "\n",
    "### Set default figure parameters for the whole notebook ###\n",
    "\n",
    "# Default parameters for matplotlib's figures.\n",
    "plt.rcParams['figure.figsize'] = [6,6]\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "#mpl.rcParams['axes.prop_cycle'] = cycler(color=['b', 'r', 'g'])\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['ytick.left'] = True\n",
    "\n",
    "# Default parameters of seaborn's figures.\n",
    "sns.set_style('white') # NB: Needs to be above sns.set_theme to properly attend custom_params.\n",
    "custom_params = {'axes.spines.right': False, 'axes.spines.top': False}\n",
    "sns.set_theme(style='ticks', palette='deep', rc=custom_params)\n",
    "\n",
    "\n",
    "# Global file paths.\n",
    "#EXPORTS_DIR_PATH = 'Exports'\n",
    "EXPORTS_MODELS_DIR_PATH = r'Exports\\Models\\Tried'\n",
    "IMPORTS_DIR_PATH = r'Exports\\Preprocessed_data'\n",
    "\n",
    "CSV_MODELS_FILE = 'models_info.csv'\n",
    "PKL_MODELS_FILE = 'models_info.pkl'\n",
    "#JSON_MODELS_FILE = 'models_info.json'\n",
    "#DATASETS_DIR_PATH = r'D:\\0Partage\\MP-P2PNet\\MP-Sync\\MP-Sync_Pro\\Info\\OC_DS\\Projet 7\\Datasets' #os.path.join('D:', '0Partage', 'MP-P2PNet', 'MP-Sync', 'MP-Sync_Pro', 'Info', 'OC_DS', 'Projet 7', 'Datasets')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Importation of required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Files management (Save and load files).\\nimport csv\\nimport pickle\\n\\n# Additional common libraries.\\nfrom numpy import argmax, argmin\\nimport math\\n\\n# sklearn tools ad libraries.\\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_predict, cross_val_score\\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, precision_recall_curve, fbeta_score, confusion_matrix\\n\\n# Make a sklearn job scorer.\\nfrom sklearn.metrics import make_scorer\\n\\n# Data sampling.\\nfrom imblearn.pipeline import Pipeline # NB: imbalearn.pipeline.Pipeline allows to properly deal the SMOTE on the train set and avoid the validation/test sets.\\nfrom sklearn.preprocessing import StandardScaler\\nfrom imblearn.over_sampling import SMOTENC # NB: SMOTENC can manage categorial features while SMOTE cannot.\\n\\n\\n### Beyesian hyperparmaters tuning ###\\n\\n# Hyperopt modules.\\nfrom hyperopt import STATUS_OK # Check if the objective function returned a valid value (Mandatory).\\n\\n# Methods for the domain space, algorithm optimization, save the trials history, bayesian optimization.\\nfrom hyperopt import hp, tpe, Trials, fmin, pyll'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Files management (Save and load files).\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "# Additional common libraries.\n",
    "from numpy import argmax, argmin\n",
    "import math\n",
    "\n",
    "# sklearn tools ad libraries.\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, precision_recall_curve, fbeta_score, confusion_matrix\n",
    "\n",
    "# Make a sklearn job scorer.\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Data sampling.\n",
    "from imblearn.pipeline import Pipeline # NB: imbalearn.pipeline.Pipeline allows to properly deal the SMOTE on the train set and avoid the validation/test sets.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTENC # NB: SMOTENC can manage categorial features while SMOTE cannot.\n",
    "\n",
    "\n",
    "### Beyesian hyperparmaters tuning ###\n",
    "\n",
    "# Hyperopt modules.\n",
    "from hyperopt import STATUS_OK # Check if the objective function returned a valid value (Mandatory).\n",
    "\n",
    "# Methods for the domain space, algorithm optimization, save the trials history, bayesian optimization.\n",
    "from hyperopt import hp, tpe, Trials, fmin, pyll\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NB: In order to calculate the best probability threshold the AUCROC is selected over the AUCPR since the first one focuses on the FP and FN balance (the 2 most important values to consider for this project) while the AUCPR focused exclusively on the positive (minority class) (=> FP) which does not take into account the most relevant value for this project (FN).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StratifiedKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the default cross validation method to use.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m SKF_5 \u001b[38;5;241m=\u001b[39m \u001b[43mStratifiedKFold\u001b[49m(\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# True: Allows hyperprameter tuning, False: Get the results stored from the last hyperparameters tuning.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m HT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StratifiedKFold' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the default cross validation method to use.\n",
    "SKF_5 = StratifiedKFold(5, shuffle=True, random_state=0)\n",
    "\n",
    "# True: Allows hyperprameter tuning, False: Get the results stored from the last hyperparameters tuning.\n",
    "HT = True\n",
    "\n",
    "# For imbalanced data use weight or data sampling.\n",
    "IMB_PROCESS = 'Weight' #'Resp'\n",
    "\n",
    "# Global common scaler to use.\n",
    "SCALER = MinMaxScaler()\n",
    "\n",
    "# Update the csv file containing the training information and scores of the model or not (True = update).\n",
    "GET_CSV_FILE = True\n",
    "\n",
    "# Set and initialize the main scorer used for the models comparisons.\n",
    "MAIN_SCORER_TRAIN_LABEL = 'Job_score_train'\n",
    "MAIN_SCORER_TEST_LABEL = 'Job_score_test'\n",
    "MAIN_SCORER_VAL = 0\n",
    "\n",
    "# Load/create and initialize the dataframe in which store all relevant models' information (best hyperparameters, scores...).\n",
    "# NB: In case of the creation of the file data=np.full((1,len(l_COL_LABELS)), None) to force dtypes as objects\n",
    "#     until one of the next added entries (rows) are full then, it will be removed. Otherwise, the np.nan values which will appear\n",
    "#     within the first row will convert their columns' dtypes to float64 and prevent their replacement\n",
    "#     by objects such as np.array.\n",
    "l_COL_LABELS = ['Model_labels', 'Models',\n",
    "                'yhat_train', 'yhat_test',\n",
    "                'Best_proba_threshold_train', 'Best_proba_threshold_test',\n",
    "                'Job_score_train', 'Job_score_test', \n",
    "                'AUROC_scores_train', 'AUROC_scores_test',\n",
    "                'F-bêta_score_train', 'F-bêta_score_test',\n",
    "                'Process_time_train (s)', 'Process_time_test (s)'\n",
    "               ]\n",
    "\n",
    "#l_COL_LABELS = ['Model_labels', 'Models',\n",
    "#                'X_train_shape', 'X_test_shape',\n",
    "#                'yhat_train', 'yhat_test',\n",
    "#                'Best_proba_threshold_train', 'Best_proba_threshold_test',\n",
    "#                'Job_score_train', 'Job_score_test', \n",
    "#                'AUROC_scores_train', 'AUROC_scores_test',\n",
    "#                'F-bêta_score_train', 'F-bêta_score_test',\n",
    "#                'Process_time_train (s)', 'Process_time_test (s)'\n",
    "#               ]\n",
    "\n",
    "if GET_CSV_FILE:\n",
    "    try:\n",
    "        df_MODELS = pd.read_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))#.set_index('Model_labels')\n",
    "\n",
    "    except:\n",
    "        print(\"No csv models informations were found. A new one is created...\")\n",
    "        df_MODELS = pd.DataFrame(data=np.full((1,len(l_COL_LABELS)), None), columns=l_COL_LABELS).set_index('Model_labels')\n",
    "        df_MODELS.to_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))\n",
    "        print('Done !')\n",
    "    \n",
    "else:\n",
    "    print(\"Creation of a new csv file to store models informations...\")\n",
    "    df_MODELS = pd.DataFrame(data=np.full((1,len(l_COL_LABELS)), None), columns=l_COL_LABELS).set_index('Model_labels')\n",
    "    df_MODELS.to_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))\n",
    "    print('Done !')  \n",
    "\n",
    "display(df_MODELS.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_EZ (x, max_rows = 100, max_cols = 100, max_colwidth = 100):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Allows to display pandas dataframes with the number of rows and columns whished in an easy manner.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame()\n",
    "        Dataframe to display.\n",
    "    max_rows: int\n",
    "        Maximum number of rows to display.\n",
    "    max_cols: int\n",
    "        Maximum number of columns to display.\n",
    "    max_colwidth: int\n",
    "        Maximum width of each column.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    with pd.option_context('display.max_rows', max_rows, 'display.max_columns', max_cols, 'display.max_colwidth', max_colwidth):\n",
    "        display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_csv_full (df):\n",
    "    \n",
    "    # Set the numpy array number of items cutting threshold to a very high number and avoid the cut.\n",
    "    set_printoptions(threshold=1e100, linewidth=1e100)\n",
    "    \n",
    "    # Save the df to a csv file.\n",
    "    df.to_csv(os.path.join(EXPORTS_MODELS_DIR_PATH, CSV_MODELS_FILE))\n",
    "    \n",
    "    # Reset the numpy array number of items cutting threshold to default.\n",
    "    set_printoptions(threshold=100, linewidth=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Importation of the preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory(df):\n",
    "    \"\"\"Reduce memory usage of a dataframe by setting data types. \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Initial df memory usage is {:.2f} MB for {} columns'\n",
    "          .format(start_mem, len(df.columns)))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            cmin = df[col].min()\n",
    "            cmax = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # Can use unsigned int here too\n",
    "                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    memory_reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print('Final memory usage is: {:.2f} MB - decreased by {:.1f}%'.format(end_mem, memory_reduction))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_int_cols (df):\n",
    "      \n",
    "    for col in df.columns:\n",
    "        for item in df[col]:\n",
    "            if item == int(item):\n",
    "                if df[col].dtype != \"int64\":\n",
    "                    df[col] = df[col].astype(\"int64\")\n",
    "            else:\n",
    "                if df[col].dtype != \"float64\":\n",
    "                    df[col] = df[col].astype(\"float64\") \n",
    "                break\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Model fitting and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_predict (model, X=X_TRAIN, y=y_TRAIN, cv=SKF_5):\n",
    "    \n",
    "    # Fit and predict probabilities by cross validation.\n",
    "    if cv != 0:\n",
    "        t0 = time()\n",
    "        y_pred_proba_NP = cross_val_predict(model, X, y, cv=cv, method='predict_proba')\n",
    "        process_time = time() - t0\n",
    "\n",
    "    # Fit and predict over the test set (no cv).\n",
    "    else:\n",
    "        t0 = time()\n",
    "        model.fit(X, y)\n",
    "        y_pred_proba_NP = model.predict_proba(X)\n",
    "        process_time = time() - t0\n",
    "    \n",
    "    return y_pred_proba_NP, process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_pred_list (y_pred_proba_P, l_proba_thrs = np.linspace(0, 1, num=100)):\n",
    "    \n",
    "    # Classify the customers of the sample for each probability threshold.\n",
    "    l_y_pred = []\n",
    "    for proba_thr in l_proba_thrs:\n",
    "\n",
    "        y_pred = []\n",
    "        for proba in y_pred_proba_P:\n",
    "            if proba > proba_thr:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "        \n",
    "        l_y_pred.append(y_pred)\n",
    "                \n",
    "    return l_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp_fp_fn_tn_lists (y_true, l_y_pred):\n",
    "\n",
    "    l_tp = []\n",
    "    l_fp = []\n",
    "    l_fn = []\n",
    "    l_tn = []\n",
    "    for y_pred in l_y_pred:\n",
    "        #cm = confusion_matrix(y_true, y_pred)\n",
    "        #l_tn.append(cm[0][0])\n",
    "        #l_fp.append(cm[0][1])\n",
    "        #l_fn.append(cm[1][0])\n",
    "        #l_tp.append(cm[1][1])\n",
    "        #display(y_true[y_true == 1])\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        l_tp.append(tp)\n",
    "        l_fp.append(fp)\n",
    "        l_fn.append(fn)\n",
    "        l_tn.append(tn)\n",
    "\n",
    "    \n",
    "    return l_tp, l_fp, l_fn, l_tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Optimization of the probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_proba_thr (np_tp, np_fp, np_fn, np_tn, l_proba_thrs, fn_cost_coeff = 10):     \n",
    "        \n",
    "    ### Calculation of the best probability threshold.\n",
    "    \n",
    "    # Method 1 (Less accurate at low number of thresholds tried): Get the index of the closest FP/FN ratio of 1.\n",
    "    #J = np_fp / (fn_cost_coeff * np_fn)\n",
    "    #j_opt = 0\n",
    "    #for idx, j in enumerate(J):\n",
    "    #    if (j < 1 and j > j_opt) or (j > 1 and j < j_opt):\n",
    "    #        j_opt = j\n",
    "    #        J_opt_idx = idx\n",
    "    #print(\"Corresponding optimal ratio found:\", J[J_opt_idx])\n",
    "    \n",
    "    # Method 2 (More accurate at low number of thresholds tried):\n",
    "    # NB: As it had been seen within the global EDA notebook, there is 1 FN for 10 FP.\n",
    "    #     Our manager suggested that we should consider that 1 FN cost ~10 times more then 1 FP.\n",
    "    #     => No need to add a coefficient in front of FN to add more weight since it is already\n",
    "    #        taken into account within the classification balancement for the 2nd method.\n",
    "    #        => fn_cost_coeff = 10 --> 1\n",
    "    \n",
    "    tpr = np_tp / (np_tp + fn_cost_coeff/fn_cost_coeff * np_fn)\n",
    "    fpr = np_fp / (np_fp + np_tn)\n",
    "    J = tpr - fpr\n",
    "    J_opt_idx = argmax(J)\n",
    "    \n",
    "    # Method 3:\n",
    "    #np_fp_hypothesis = np_fp + fn_cost_coeff * np_fn\n",
    "    #J_opt_idx = argmin(np_fp_hypothesis)\n",
    "    \n",
    "    # Get the optimal probability threshold.\n",
    "    best_thr = l_proba_thrs[J_opt_idx]\n",
    "\n",
    "    # Print scores.\n",
    "    #print('Best Threshold: %.3f' % best_thr)\n",
    "    #print('Number of FP =', np_fp[J_opt_idx])\n",
    "    #print('Number of FN =', np_fn[J_opt_idx], '~ FP =', 10 * np_fn[J_opt_idx])\n",
    "    #print('Equivalent number of FP =', np_fp_hypothesis[J_opt_idx])\n",
    "    \n",
    "    return best_thr, J_opt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_density (y_true, y_pred_proba_P, best_thr):\n",
    "    \n",
    "    # Find and show the optimal probability threshold on a figure.\n",
    "    \n",
    "    # Find the best threshold value graphically.\n",
    "    y_true_P = y_true[y_true == 1]\n",
    "    y_true_N = y_true[y_true == 0]\n",
    "\n",
    "    # Plot the probability density approximation of the TN.\n",
    "    #plt.hist(y_pred_proba_P[y_true_N.index], bins=100, density=True)\n",
    "    kde_N = sns.kdeplot(y_pred_proba_P[y_true_N.index], fill=True, alpha=0.5, edgecolor='k') #multiple=\"stack\"\n",
    "    \n",
    "    # Plot the probability density approximation of the FN.\n",
    "    #plt.hist(y_pred_proba_P[y_true_P.index], bins=100, density=True)\n",
    "    sns.kdeplot(y_pred_proba_P[y_true_P.index], fill=True, alpha=0.5, edgecolor='k')\n",
    "\n",
    "    # Plot a line at the best threshold found.\n",
    "    plt.vlines(best_thr, ymin=0, ymax=max(kde_N.get_yticks()), colors='k', linestyles='--')\n",
    "    \n",
    "    # Set other figures' parameters.\n",
    "    plt.title(\"Distribution of the probability a customer default\")\n",
    "    plt.xlabel(\"Probability thresholds\")\n",
    "    plt.legend([\"Regular customers\", \"Default customers\"])\n",
    "    \n",
    "    # Draw the figure.\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_sum_fp_coeff_fn (np_fp, np_fn, l_proba_thrs, best_thr, fn_cost_coeff = 10):\n",
    "    \n",
    "    # Apply the cost hypothesis and convert FN to its supposed corresponding number of FP.\n",
    "    np_fp_hypothesis = np_fp + fn_cost_coeff * np_fn\n",
    "    \n",
    "    # Plot the corresponding curve.\n",
    "    plt.plot(l_proba_thrs, np_fp_hypothesis)\n",
    "     \n",
    "    # Plot a line at the best threshold found.\n",
    "    plt.vlines(best_thr, ymin=0, ymax=max(np_fp_hypothesis), colors='k', linestyles='--')\n",
    "    \n",
    "    # Set other figures' parameters.\n",
    "    plt.title(\"Total number of corresponding FP according to probability thresholds\")\n",
    "    plt.xlabel(\"Probability thresholds\")\n",
    "    plt.ylabel(\"Total false positives\")\n",
    "    \n",
    "    # Draw the figure.\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_roc (y_true, l_yhats, l_model_labels):\n",
    "    \n",
    "    idx = 0\n",
    "    for idx in range(len(l_model_labels)):\n",
    "        model_label = l_model_labels[idx]\n",
    "        yhat = l_yhats[idx]\n",
    "            \n",
    "        # Calculate inputs for the roc curve.\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, yhat)\n",
    "        \n",
    "        # Calculate the corresponding AUC.\n",
    "        auroc = roc_auc_score(y_true, yhat)\n",
    "    \n",
    "        # Plot the roc curves.\n",
    "        plt.plot(fpr, tpr, marker='.', markersize=2, label=model_label + \" (AUC = %.3f)\" % auroc)\n",
    "        \n",
    "        # Iterate the index value for the next loop.\n",
    "        idx += 1\n",
    "    \n",
    "    # Plot the no skill roc curve (the diagonal line).\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='No skill (AUC = 0.5)', color='k', alpha=0.75)\n",
    "    \n",
    "    # Set axis labels and the title.\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC\")\n",
    "    \n",
    "    # Show the legend.\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) F-bêta score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fbeta_score (l_proba_thrs, l_fbeta, beta, best_thr, best_thr_idx):\n",
    "       \n",
    "    # Get the optimal F-bêta score.\n",
    "    print('F-Bêta score of the optimal threshold found = %.3f' % l_fbeta[best_thr_idx])\n",
    "    print('Highest F-Bêta score = %.3f' % max(l_fbeta))\n",
    "    \n",
    "    return max(l_fbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_fbeta_score (l_proba_thrs, l_fbeta, best_thr):\n",
    "    \n",
    "    # Plot the graph.\n",
    "    plt.plot(l_proba_thrs, l_fbeta)\n",
    "    \n",
    "    # Plot a line at the best threshold found.\n",
    "    plt.vlines(best_thr, ymin=0, ymax=max(l_fbeta), colors='k', linestyles='--')\n",
    "     \n",
    "    # Set other figures' parameters.\n",
    "    plt.title('F-bêta score = f(Probability thresholds)')\n",
    "    plt.xlabel('Probability thresholds')\n",
    "    plt.ylabel('F-bêta score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Job score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_norm (y_true, y_pred, fn_value=-10, fp_value=0, tp_value=0, tn_value=1):\n",
    "\n",
    "\n",
    "    # Get the confusion matrix values.\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # gain total\n",
    "    g = tp*tp_value + tn*tn_value + fp*fp_value + fn*fn_value\n",
    "    # gain maximum\n",
    "    g_max = (fp + tn)*tn_value + (fn + tp)*tp_value\n",
    "    # gain minimum\n",
    "    g_min = (fp + tn)*fp_value + (fn + tp)*fn_value\n",
    "    # gain normalisé\n",
    "    g_norm = (g - g_min)/(g_max - g_min)\n",
    "\n",
    "    return g_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_w_thr_fct (y_true, y_pred_proba_P, scorer = 'g_norm', verbose = True):\n",
    "    \n",
    "    \"\"\" Function which make the scorer take into account the best probability threshold found for the current set of hyperparameters. \"\"\"\n",
    "    \n",
    "    # Set the probability threshold range to try.\n",
    "    l_proba_thrs = np.linspace(0, 1, num=101)\n",
    "    \n",
    "    # Get all set of predictions for each threshold.\n",
    "    l_y_pred = get_y_pred_list(y_pred_proba_P)\n",
    "    \n",
    "    # Get the confusion matrix results for each set of predictions.\n",
    "    np_tp, np_fp, np_fn, np_tn = np.array(get_tp_fp_fn_tn_lists(y_true, l_y_pred))\n",
    "    \n",
    "    # Get the best threshold and its corresponding index with the list of the predictions sets.\n",
    "    best_thr, idx = opt_proba_thr(np_tp, np_fp, np_fn, np_tn, l_proba_thrs, fn_cost_coeff = 10)\n",
    "    best_thr = round(best_thr, 2)\n",
    "\n",
    "    # Select the prediction set corresponding to the best threshold found.\n",
    "    y_pred = l_y_pred[idx+1]\n",
    "\n",
    "    \n",
    "    # Select the scorer to use.\n",
    "    if scorer == 'g_norm':\n",
    "        # Calculate the normalized gain.\n",
    "        score = gain_norm(y_true, y_pred, fn_value=-10, fp_value=0, tp_value=0, tn_value=1)\n",
    "    elif scorer == 'auroc':\n",
    "        score = roc_auc_score(y_true, y_pred)\n",
    "    elif scorer == 'fbeta':\n",
    "        score = f_beta_score(y_true, y_pred, beta=3)\n",
    "        \n",
    "    \n",
    "    if verbose:\n",
    "        print('Best probability threshold:', best_thr)\n",
    "\n",
    "    return score, best_thr\n",
    "\n",
    "# Make the function as a new scorer for sklearn.\n",
    "g_norm_scorer = make_scorer(scorer_w_thr_fct, scorer = 'g_norm', verbose=False)#, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tune_rand_grid (model, X, y, para_grid, score, cv, score_label = 'score',\n",
    "                          n_iter = 100, grid_loop = 1, range_precision = None, rand_state = 0, verbose = 0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Tune the chosen hyperapameters of the model by checking a random number of combinations with the RandomizedSearchCV() method.\n",
    "    This function tunes the hyperparameters by taking the best combination among the best one found in each loop run.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: sklearn model\n",
    "        Model to test.\n",
    "    X: pandas.Dataframe()\n",
    "        Dataframe of the explicatives.\n",
    "    y: pandas.Dataframe()\n",
    "        Dataframe of the explicated.\n",
    "    n_folds: int\n",
    "        Split parameter of the KFold function.\n",
    "    para_grid: dictionary\n",
    "        Parameters of the model to tune.\n",
    "    n_iter: int\n",
    "        Number of combinations of hyperparameters within which the RandomizedSearchCV will pick the best.\n",
    "    grid_loop:\n",
    "        Number of loop run.\n",
    "    range_precision: int\n",
    "        Number of decimals to round to.\n",
    "    int_para_names: list of strings\n",
    "        Hyperparameters which accept only integers.\n",
    "    bool_para_names: list of strings\n",
    "        Hyperparameters which accept only booleans.\n",
    "    rand_state: int\n",
    "        random_state parameter in order to fix the randomness of the runs.\n",
    "\n",
    "    Return: sklearn.RandomizedSearchCV()\n",
    "    ------\n",
    "    Returns the fitted randomized model grid.\n",
    "        \n",
    "    \"\"\"      \n",
    "    \n",
    "    def show ():\n",
    "        \n",
    "        \"\"\"\n",
    "        Description\n",
    "        -----------\n",
    "        Show relevant information at the end of the process.\n",
    "        \n",
    "        \"\"\"  \n",
    "        \n",
    "        # Fit the grid model.\n",
    "        model_grid.fit(X, y)\n",
    "        \n",
    "        # Display the best hyperparameter.\n",
    "        print(\"\\nBest estimator found:\\n\", model_grid.best_estimator_)\n",
    "        print(\"\\nBest score found:\\n\", score_label, '=', round(model_grid.best_score_, 3))\n",
    "        print(\"\\nBest hyperparameters found:\\n\", model_grid.best_params_)  \n",
    "\n",
    "        \n",
    "    # Set the KFold cross validation with the selected n_folds.\n",
    "    #skf = StratifiedKFold(n_folds, shuffle=True, random_state=0)\n",
    "\n",
    "    # Create a dictionary with all parameters to test as keys and empty lists as values.\n",
    "    best_para_stored = para_grid.copy()\n",
    "    for key in best_para_stored.keys():\n",
    "        best_para_stored[key] = []\n",
    "    \n",
    "    for i in range(grid_loop):\n",
    "               \n",
    "        # Random search of parameters, use all available cores.\n",
    "        model_grid = RandomizedSearchCV(estimator=model, param_distributions=para_grid, cv=cv, scoring=score,\n",
    "                                        n_iter=n_iter, refit=True, n_jobs=-1, random_state=rand_state, verbose=verbose)\n",
    "         \n",
    "        # Fit the random grid.\n",
    "        # NB: Step needed to be able to get the \"best_params_\" method.\n",
    "        model_grid.fit(X, y)#.to_numpy().ravel())\n",
    "\n",
    "        # Get the best parameters values in a dictionary.\n",
    "        best_para = model_grid.best_params_\n",
    "        \n",
    "        # Loop to store the best parameter got in this loop run, in order to make the average at the end of all runs.\n",
    "        for key in best_para_stored.keys():\n",
    "            \n",
    "            # Get the best value for the \"key\" parameter.\n",
    "            if range_precision != None:\n",
    "                best_para_value = round(best_para.get(key), range_precision)\n",
    "            else:\n",
    "                best_para_value = best_para.get(key)\n",
    "            \n",
    "            # Store this value in the dictionary set at the beginning of the function (\"best_para_stored\").\n",
    "            best_para_stored[key].append(best_para_value)\n",
    "            \n",
    "            # Remove duplicates.\n",
    "            best_para_stored[key] = list(set(best_para_stored[key]))\n",
    "    \n",
    "\n",
    "    # Replace the initial parameters with the best ones found.\n",
    "    para_grid = best_para_stored\n",
    "    \n",
    "    # Find the best parameter among the best found.\n",
    "    model_grid = GridSearchCV(estimator=model, param_grid=para_grid, cv=cv, scoring=score, n_jobs=-1, verbose=verbose)\n",
    "    \n",
    "    # Show results.\n",
    "    show()\n",
    "        \n",
    "    return model_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tune_bayes (model,\n",
    "                      search_space,\n",
    "                      model_label,\n",
    "                      cv,\n",
    "                      action = 0,\n",
    "                      max_eval = 5,\n",
    "                      eval_metric = gain_norm, # Mesure d'évaluation\n",
    "                      eval_metric_label = 'g_norm',\n",
    "                      X = X_TRAIN,\n",
    "                      y = y_TRAIN,\n",
    "                      n_iter = 1\n",
    "                     ):\n",
    "\n",
    "    \n",
    "    ### Configuration ###\n",
    "\n",
    "    # Set global variabls.\n",
    "    global ITERATION\n",
    "    \n",
    "    # Set save file path.\n",
    "    history_dir_path = os.path.join(r'Exports\\Models\\Selected\\Hyperparams_tuning_history')\n",
    "    csv_file_name = model_label + \"_\" + eval_metric_label + \"_trials.csv\"\n",
    "    p_file_name = model_label + \"_\" + eval_metric_label + \"_trials.pkl\"\n",
    "    \n",
    "    \n",
    "    #history_json_file_path = os.path.join(r'Exports\\Models\\Hyperparams_tuning_history',\n",
    "    #                                 model_label + \"_\" + eval_metric_label + '_trials.json')\n",
    "    \n",
    "    \n",
    "    # 1. Definition of the objective function (=> calculation of the loss score).\n",
    "    def objective (hyperparams_set):\n",
    "        \n",
    "        print(hyperparams_set)\n",
    "        \n",
    "        # Count each function call as an iteration.\n",
    "        global ITERATION    \n",
    "        ITERATION += 1\n",
    "\n",
    "        # On s'assure que les paramètres soient au bon format\n",
    "        #for param_label in ['num_leaves','max_depth','n_estimators']:\n",
    "        #    params[param_label] = int(params[param_label])\n",
    "\n",
    "        ### Initializes a new set of model's hyperparameters values.\n",
    "        #model_hyperparams = {'n_estimators': int(params['n_estimators']), \n",
    "        #                     'class_weight': params['class_weight'],\n",
    "        #                     'max_depth': int(params['max_depth']), \n",
    "        #                     'learning_rate': params['learning_rate'],\n",
    "        #                     'subsample': params['subsample'],\n",
    "        #                     'colsample_bytree': params['colsample_bytree'],\n",
    "        #                     'num_leaves': int(params['num_leaves']),\n",
    "        #                     'reg_alpha': params['reg_alpha'],\n",
    "        #                     'reg_lambda': params['reg_lambda']\n",
    "        #                    }\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Methode 1: The best threshold is determined by the algorithm as any other hyperparmeters ###\n",
    "        # NB: 25 % faster than method 2.\n",
    "        \n",
    "        # Get the classification probability threshold to try (chosen the algorithm).\n",
    "        #proba_thr = hyperparams_set['proba_thr']\n",
    "        \n",
    "        # Delete the custom probability \"hyperparmeter\" as it does not belong to the model hyperparameters.\n",
    "        #del hyperparams_set['proba_thr']\n",
    "        \n",
    "        # Set the new model's hyperparameters values to try.\n",
    "        #model.set_params(**hyperparams_set)\n",
    "\n",
    "        # Get the time before the model training and predictions processes.\n",
    "        #t0 = time()\n",
    "        \n",
    "        # Get the positive probabilities over all validation folds.\n",
    "        #y_proba_P_cv = cross_val_predict(model, X, y, method='predict_proba', cv=cv)[:, 1]\n",
    "\n",
    "        # If positive probability > probility threshold => prediction = 1\n",
    "        # NB: \"(y_proba_P_cv > proba_thr)\" returns a boolean np.array() and \"* 1\" converts booleans as True = 1 or False = 0.\n",
    "        #y_pred = (y_proba_P_cv > proba_thr) * 1\n",
    "            \n",
    "        # Get the score value of the chosen metric.\n",
    "        #score = eval_metric(y, y_pred)\n",
    "        \n",
    "        \n",
    "        ### Methode 2: The best threshold is determined at each iteration by\n",
    "        #              calculating all thresholds before selecting the best one. \n",
    "        #              Everything is done within the score calculation function ###\n",
    "        # NB: 25 % slower than method 1.\n",
    "        \n",
    "        # Set the new model's hyperparameters values to try.\n",
    "        model.set_params(**hyperparams_set)\n",
    "\n",
    "        # Get the time before the model training and predictions processes.\n",
    "        t0 = time()\n",
    "        \n",
    "        # Get the positive probabilities over all validation folds.\n",
    "        y_proba_P_cv = cross_val_predict(model, X, y, method='predict_proba', cv=cv)[:, 1]\n",
    "\n",
    "        # Get the score value of the chosen metric.\n",
    "        score, proba_thr = eval_metric(y, y_proba_P_cv)\n",
    "    \n",
    "        \n",
    "        ### Calculate the loss function ###\n",
    "    \n",
    "        # Calculate the corresponding loss (value to minimize).\n",
    "        loss = 1 - score\n",
    "\n",
    "        # Calculate the processing time.\n",
    "        run_time = round((time() - t0), 2)\n",
    "               \n",
    "        # Save the trials results into a readable csv.\n",
    "        with open(os.path.join(history_dir_path, csv_file_name), 'a') as history_file:\n",
    "            csv_file_writer = csv.writer(history_file)\n",
    "            csv_file_writer.writerow([ITERATION, loss, proba_thr, run_time, hyperparams_set])\n",
    "            history_file.close()\n",
    "            \n",
    "        #with open(os.path.join(history_dir_path, p_file_name), 'a') as history_file:\n",
    "            #json_file_writer = json.writer(history_file)\n",
    "            #history_file.write(json.dumps({'loss': loss, 'status': STATUS_OK}))\n",
    "            #history_file.close()\n",
    "        \n",
    "        return {'loss': loss, 'status': STATUS_OK} #, 'hyperparams': hyperparams_set, 'proba_thr': proba_thr, 'iter': ITERATION, 'run_time': run_time,}\n",
    "\n",
    "    \n",
    "    def process ():\n",
    "        \n",
    "        best_hyperparams = fmin(fn = objective,\n",
    "                                space = search_space,\n",
    "                                algo = tpe.suggest,\n",
    "                                max_evals = max_eval,\n",
    "                                trials = trials,\n",
    "                #               rstate = np.random.RandomState(rs_)\n",
    "                               )       \n",
    "        \n",
    "        # Save the trials' history to continue the hyperparameter tuning if required later on.\n",
    "        #history_file = pd.read_csv(os.path.join(history_dir_path, csv_file_name))\n",
    "        #json.dump(trials, open(os.path.join(history_dir_path, p_file_name)), indent=4)\n",
    "        pickle.dump(trials, open(os.path.join(history_dir_path, p_file_name), \"wb\"))   \n",
    "       \n",
    "    \n",
    "    # 2. Dictionary of hyperparameters (Domain space).\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # 3. Optimization algorithm (Substitution function).\n",
    "    tpe_algorithm = tpe.suggest\n",
    "    \n",
    "    \n",
    "    # 4. Process and save trials.\n",
    "            \n",
    "    if action == 0:\n",
    "               \n",
    "        # To save trials.\n",
    "        trials = Trials()\n",
    "\n",
    "        # Create and open the file into which save the history of the hyperparameter tuning.\n",
    "        with open(os.path.join(history_dir_path, csv_file_name), 'w') as history_file:\n",
    "\n",
    "            csv_file_writer = csv.writer(history_file)\n",
    "\n",
    "            # Create the header of the file then close it.\n",
    "            csv_file_writer.writerow(['iter', 'loss', 'proba_thr', 'run_time', 'tuned_hyperparams'])\n",
    "            history_file.close()\n",
    "\n",
    "        # Create and open the file into which save the history of the hyperparameter tuning.\n",
    "        with open(os.path.join(history_dir_path, p_file_name), 'w') as history_file:\n",
    "            history_file.close()\n",
    "\n",
    "\n",
    "        #global ITERATION\n",
    "        ITERATION = 0\n",
    "\n",
    "        process()\n",
    "        \n",
    "        \n",
    "        ### Adjustements in case of n_iter > 1 ###\n",
    "        \n",
    "        # Remove one iteration if the file is created instead of loaded.\n",
    "        n_iter -= 1\n",
    "        \n",
    "        # Tell the algorithm to write in the created files the nexts results instead of overwritting it.\n",
    "        action = 1\n",
    "\n",
    "        \n",
    "    if action == 1 and n_iter > 0:\n",
    "    \n",
    "        add_n_eval = max_eval\n",
    "    \n",
    "        # Process the hyperparamters tuning as many time as set.\n",
    "        for i in range(n_iter):\n",
    "                \n",
    "            # Save the trial results\n",
    "            #with open('trials.json', 'w') as f:\n",
    "               # f.write(json.dumps(trials_dict))\n",
    "\n",
    "            trials = pickle.load(open(os.path.join(history_dir_path, p_file_name), \"rb\"))\n",
    "            #trials = json.load(open(os.path.join(history_dir_path, p_file_name)))\n",
    "            df_history = pd.read_csv(os.path.join(history_dir_path, csv_file_name))\n",
    "\n",
    "            #trials = pickle.load(open(\"my_model.hyperopt\", \"rb\"))x\n",
    "            print(\"Trials' history loading...\")\n",
    "            max_eval = len(trials.trials) + add_n_eval\n",
    "            print(\"Running from the {}th trial to {} trials (=> +{} trials)\".format(len(trials.trials), max_eval, add_n_eval))\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            ITERATION = len(df_history)\n",
    "\n",
    "            process()\n",
    "\n",
    "    df_history = pd.read_csv(os.path.join(history_dir_path, csv_file_name)).sort_values(by='loss')\n",
    "\n",
    "    return df_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) Model peformance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (y_pred_proba_P, y_true = y_TEST, fig = (1,1,1,1), l_model_labels = ['Model']):\n",
    "      \n",
    "    global MAIN_SCORER_VAL\n",
    "    \n",
    "    ### Calculate necessary variables.\n",
    "    \n",
    "    # List of the probability thresholds to try.\n",
    "    l_proba_thrs = np.linspace(0, 1, num=201)\n",
    "    \n",
    "    # Get the predictions corresponding to each probability thresholds tried.\n",
    "    l_y_pred = get_y_pred_list(y_pred_proba_P, l_proba_thrs)\n",
    "    \n",
    "    # Get the corresponding TP, FP, FN and TN for each probability thresholds tried.\n",
    "    np_tp, np_fp, np_fn, np_tn = np.array(get_tp_fp_fn_tn_lists(y_true, l_y_pred))\n",
    "    \n",
    "    # FN cost coefficient (FN ~ 10 FP).\n",
    "    fn_cost_coeff = 10\n",
    "    \n",
    "    # Display figures configuration.\n",
    "    n_fig = fig.count(1)\n",
    "    \n",
    "    if n_fig != 0:\n",
    "        plt.figure(figsize=(12,12), dpi=300)\n",
    "        p = 0\n",
    "        if n_fig == 1:\n",
    "            l = 1; c = 1\n",
    "        elif n_fig == 2:\n",
    "            l = 1; c = 2\n",
    "        elif n_fig >= 3:\n",
    "            l = 2; c = 2  \n",
    "    \n",
    "    \n",
    "    ### Calculate the optimal probability threshold.\n",
    "    \n",
    "    # Calculate the optimal threshold.\n",
    "    model_best_thr, best_thr_idx = opt_proba_thr(np_tp, np_fp, np_fn, np_tn, l_proba_thrs)    \n",
    "    \n",
    "    # Plot figures.\n",
    "    if fig[0]:\n",
    "        p += 1 \n",
    "        plt.subplot(l,c,p)\n",
    "        figure_density(y_true, y_pred_proba_P, model_best_thr)\n",
    "    \n",
    "    if fig[1]:\n",
    "        p += 1 \n",
    "        plt.subplot(l,c,p)\n",
    "        figure_sum_fp_coeff_fn(np_fp, np_fn, l_proba_thrs, model_best_thr, fn_cost_coeff)\n",
    "    \n",
    "    \n",
    "    ### Calculate scores.\n",
    "    \n",
    "    # ROC AUC score.\n",
    "    roc_auc_s = roc_auc_score(y_true, y_pred_proba_P)\n",
    "    print('\\nROC-AUC = %f' % roc_auc_s) #%.3f\n",
    "    \n",
    "    # Plot figure.\n",
    "    if fig[2]:\n",
    "        p += 1 \n",
    "        plt.subplot(l,c,p)\n",
    "        figure_roc(y_true, [y_pred_proba_P], l_model_labels)\n",
    "    \n",
    "    \n",
    "    # F-bêta score.\n",
    "    # NB: Square beta = cost FN / cost FP = 10\n",
    "    square_beta = 100\n",
    "    beta = round(math.sqrt(square_beta), 2)\n",
    "      \n",
    "    # Calculate the F-bêta score for each probability thresholds tried.\n",
    "    l_fbeta = []\n",
    "    for y_pred in l_y_pred:\n",
    "        fbeta = fbeta_score(y_true, y_pred, beta=beta)\n",
    "        l_fbeta.append(fbeta)\n",
    "    fbeta = get_fbeta_score(l_proba_thrs, l_fbeta, beta, model_best_thr, best_thr_idx)\n",
    "\n",
    "    # Plot figure.\n",
    "    if fig[3]:\n",
    "        p += 1 \n",
    "        plt.subplot(l,c,p)\n",
    "        figure_fbeta_score(l_proba_thrs, l_fbeta, model_best_thr)\n",
    "    \n",
    "    \n",
    "    # Job score.\n",
    "    g_norm = gain_norm(y_true, l_y_pred[best_thr_idx])\n",
    "    MAIN_SCORER_VAL = g_norm\n",
    "    print(\"Job score: %.3f\" % g_norm)\n",
    "\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 100 + \"\\n\")\n",
    "    # Draw figures.\n",
    "    if fig != (0,0,0,0):\n",
    "        plt.show() \n",
    "        \n",
    "    return model_best_thr, g_norm, roc_auc_s, fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cv (model, X = X_TRAIN, y = y_TRAIN, l_scores = ['roc_auc', 'precision', 'recall'], cv = 5):\n",
    "    \n",
    "    cv_folds_results = cross_validate(model, X, y, cv=cv, scoring=l_scores, return_train_score=True)\n",
    "    \n",
    "    display(cv_folds_results)\n",
    "    \n",
    "    cv_mean_results = {}\n",
    "    for key, value in cv_folds_results.items():\n",
    "        cv_mean_results[key] = np.mean(value) #sum(folds_scores) / float(len(folds_scores))\n",
    "    \n",
    "    display(cv_mean_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h) Table to store all models' relevant values along the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizing_table (df, l_vars, eval_dataset):\n",
    "    \n",
    "    ### Variables unpacking ###\n",
    "    model_label_key, model_key, \\\n",
    "    yhat_train_key, yhat_test_key, \\\n",
    "    best_thr_train_key, best_thr_test_key, \\\n",
    "    g_norm_train_key, g_norm_test_key, \\\n",
    "    rocauc_train_key, rocauc_test_key, \\\n",
    "    fbeta_train_key, fbeta_test_key, \\\n",
    "    process_time_train_key, process_time_test_key = l_COL_LABELS\n",
    "    \n",
    "    model_label, model, yhat, best_thr, g_norm, rocauc, fbeta, process_time = l_vars\n",
    "\n",
    "    \n",
    "    ### Select if the values corresponds to the validation set or the test set ###\n",
    "    if eval_dataset == 'valid_set':\n",
    "        dict_val = {yhat_train_key: yhat,\n",
    "                    best_thr_train_key: best_thr,\n",
    "                    g_norm_train_key: g_norm,\n",
    "                    rocauc_train_key: rocauc,\n",
    "                    fbeta_train_key: fbeta,\n",
    "                    process_time_train_key: process_time\n",
    "                   }\n",
    "\n",
    "    else: # test_set\n",
    "        dict_val = {yhat_test_key: yhat,\n",
    "                    best_thr_test_key: best_thr,\n",
    "                    g_norm_test_key: g_norm,\n",
    "                    rocauc_test_key: rocauc,\n",
    "                    fbeta_test_key: fbeta,\n",
    "                    process_time_test_key: process_time\n",
    "                   }\n",
    "    \n",
    "    dict_model = {model_label_key: model_label, model_key: model}\n",
    "    \n",
    "\n",
    "    ### Create or update the right line in the dataframe ###\n",
    "    # NB: In python > 3.9 it is possible to merge dictionaries with \"|\" (the last dictionary takes the priority in conflicts).\n",
    "    \n",
    "    # Create a new row if it does not exist.\n",
    "    if model_label not in df.index:\n",
    "        print(\"Creating new entry...\")\n",
    "        df.loc[model_label] = dict_model | dict_val #[None] * df.shape[1]\n",
    "        print(\"Done!\")\n",
    "        \n",
    "    # Update the row.\n",
    "    else:\n",
    "        print(\"Updating entry...\")\n",
    "        df.loc[model_label] = df.loc[model_label].to_dict() | dict_model | dict_val \n",
    "        print(\"Done!\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sum_table (df, l_vars, get_csv_file, eval_dataset, force_update = False):\n",
    "\n",
    "    # Update the csv file if the main score is higher.\n",
    "    if get_csv_file:\n",
    "        \n",
    "        # Reload the csv file in a df (created or already loaded at the beginning of the notebook).\n",
    "        df = pd.read_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))#.set_index('Model_labels')\n",
    "        \n",
    "        # Remove the initializing row (first row of filled with None) if one of the added rows are already full.    \n",
    "        if df.index[0] == None:\n",
    "            i = 0\n",
    "            for i in range(df.shape[0]):\n",
    "                if sum(df.iloc[i].isna()) == 0:\n",
    "                    df.dropna(how='all', inplace=True) # Drop the now useless first row.\n",
    "                    #df = df.convert_dtypes() # Infere all dtypes to according to the data type under each column.\n",
    "                    break\n",
    "        \n",
    "        # Check if the measures are from the test set or the train set.\n",
    "        if eval_dataset == 'valid_set':\n",
    "            main_scorer_label = MAIN_SCORER_TRAIN_LABEL\n",
    "        else:\n",
    "            main_scorer_label = MAIN_SCORER_TEST_LABEL\n",
    "        \n",
    "        # Check if the model_label entry is in the df.\n",
    "        model_label = l_vars[0]        \n",
    "\n",
    "        if model_label not in df_MODELS.index:\n",
    "            df = summarizing_table(df, l_vars, eval_dataset)\n",
    "            #df_to_csv_full(df)\n",
    "            df.to_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))\n",
    "            print('The new informations have been saved in a new row.')\n",
    "        \n",
    "        # Check if the score is inferior from the one already stored in the csv file.\n",
    "        elif df.loc[model_label, main_scorer_label] < MAIN_SCORER_VAL or pd.isnull(df.loc[model_label, main_scorer_label]) or force_update:\n",
    "            df = summarizing_table(df, l_vars, eval_dataset)\n",
    "            #df_to_csv_full(df)\n",
    "            df.to_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))\n",
    "            print('The row have been updated.')\n",
    "        \n",
    "        # Don't update if the new score is below the one in the csv file.\n",
    "        else:\n",
    "            print('The new score is inferior to the one already saved.')\n",
    "            print('Dataframe not saved.')\n",
    "\n",
    "            \n",
    "    # Do not update the csv file (set by user).\n",
    "    else: \n",
    "        df = summarizing_table(df, l_vars, eval_dataset)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model_label (imb_process, label_root):\n",
    "    \n",
    "    if imb_process == 'Resp':\n",
    "        model_label = 'resp_' + label_root\n",
    "    elif imb_process == 'Weight':\n",
    "        model_label = 'wt_' + label_root\n",
    "    else:\n",
    "        model_label = label_root\n",
    "    \n",
    "    return model_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_pipeline (model, scaler = SCALER, imb_process = IMB_PROCESS):\n",
    "    \n",
    "    # Initiliaze the model or the pipeline with its default values.\n",
    "    # NB: scaler = MinMaxScaler() as binary categories won't be changed and\n",
    "    #     the distance between all other values of a feature will be kept.\n",
    "    pipe_vars = []\n",
    "    if scaler != None:\n",
    "        pipe_vars.append(('scaler', scaler))\n",
    "            \n",
    "    if imb_process == 'Resp':\n",
    "        pipe_vars.append(('resampler', SMOTE(random_state=0)), ('model', model))\n",
    "    else:\n",
    "        pipe_vars.append(('model', model))\n",
    "                            \n",
    "    model_pl = Pipeline(pipe_vars)\n",
    "        \n",
    "    return model_pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import copy\n",
    "\n",
    "import shap\n",
    "from scipy.special import expit # Importing the logit function for the base value transformation.\n",
    "\n",
    "shap.initjs()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logodd_to_odd (explanations, y_pred, cat_class):\n",
    "    \n",
    "    # Initialize the explanation objects which will store the transformed values.\n",
    "    explanation = copy.deepcopy(explanations)\n",
    "    explanations_transformed = copy.deepcopy(explanations)\n",
    "    \n",
    "    # Store the length of each client's shape values in logodd.\n",
    "    len_values = len(explanations[0])\n",
    "    \n",
    "    # Transform values.\n",
    "    for i in range(len(explanations.values)):\n",
    "\n",
    "        # Reformat the explanation attributes to their normal format.\n",
    "        explanation.values = explanations.values[i].reshape(1, len_values)\n",
    "                       \n",
    "        # Select the probability to untransform (Select the value corresponding to the selected category's class).\n",
    "        base_value = explanation.base_values[i]\n",
    "\n",
    "        # Compute the original_explanation_distance to construct the distance_coefficient later on.\n",
    "        original_explanation_distance = np.sum(explanation.values, axis=1)#[cat_class]\n",
    "\n",
    "        # Get the untransformed base value (Odd).\n",
    "        base_value_trf = 1 - expit(base_value) # = 1 - 1 / (1 + np.exp(-transformed_base_value))\n",
    "\n",
    "        # Compute the distance between the model_prediction and the untransformed base_value.\n",
    "        distance_to_explain = y_pred[i][cat_class] - base_value_trf\n",
    "\n",
    "        # The distance_coefficient is the ratio between both distances cat_class will be used later on.\n",
    "        distance_coefficient = original_explanation_distance / distance_to_explain\n",
    "\n",
    "        # Transform the original shapley values to the new scale (Odd scale).\n",
    "        explanations_transformed.values[i] = explanation.values / distance_coefficient        \n",
    "        \n",
    "        # Finally reset the base_value as it does not need to be transformed.\n",
    "        explanations_transformed.base_values[i] = base_value_trf       \n",
    "        \n",
    "    # Return the transformed array from the logodd to the odd scale.\n",
    "    return explanations_transformed    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shap explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Different explainers: https://snyk.io/advisor/python/shap/functions/shap.explainers.explainer.Explainer\n",
    "- Background data or not & feature_perturbation = \"interventional\" or \"tree_path_dependent\" ? https://github.com/slundberg/shap/issues/1098 <br>\n",
    "=> Background data: Closer to the model. <br>\n",
    "\n",
    "*NB: According to the model (classifier or regressor) and the presence or not of background data, some graphics (such as shap.plots.bar()) won't behave the same way and might not be usable (for the classifiers mainly). Ex: shap.plots.bar can replaced by shap.plot_bar but such graphics are less detailed (as it can be noticed in a couple of cells below).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "544px",
    "width": "676px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "347.038px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
