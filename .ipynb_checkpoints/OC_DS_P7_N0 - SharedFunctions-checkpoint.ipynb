{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 7 - Implementation of a scoring model\n",
    "# Notebook - Shared Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Projet-7---Implementation-of-a-scoring-model\" data-toc-modified-id=\"Projet-7---Implementation-of-a-scoring-model-1\">Projet 7 - Implementation of a scoring model</a></span></li><li><span><a href=\"#Notebook---Shared-Functions\" data-toc-modified-id=\"Notebook---Shared-Functions-2\">Notebook - Shared Functions</a></span></li><li><span><a href=\"#I)-Importations-and-global-settings\" data-toc-modified-id=\"I)-Importations-and-global-settings-3\">I) Importations and global settings</a></span><ul class=\"toc-item\"><li><span><a href=\"#1)-Importation-of-required-libraries\" data-toc-modified-id=\"1)-Importation-of-required-libraries-3.1\">1) Importation of required libraries</a></span></li><li><span><a href=\"#2)-Settings-of-global-graphics-parameters\" data-toc-modified-id=\"2)-Settings-of-global-graphics-parameters-3.2\">2) Settings of global graphics parameters</a></span></li><li><span><a href=\"#3)-Global-files'-path\" data-toc-modified-id=\"3)-Global-files'-path-3.3\">3) Global files' path</a></span></li></ul></li><li><span><a href=\"#II)-Functions\" data-toc-modified-id=\"II)-Functions-4\">II) Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#1)-Basics\" data-toc-modified-id=\"1)-Basics-4.1\">1) Basics</a></span></li><li><span><a href=\"#2)-Dataframes-optimization\" data-toc-modified-id=\"2)-Dataframes-optimization-4.2\">2) Dataframes optimization</a></span></li><li><span><a href=\"#3)-Model-fitting-and-predictions\" data-toc-modified-id=\"3)-Model-fitting-and-predictions-4.3\">3) Model fitting and predictions</a></span></li><li><span><a href=\"#4)-Optimization-of-the-probability-threshold\" data-toc-modified-id=\"4)-Optimization-of-the-probability-threshold-4.4\">4) Optimization of the probability threshold</a></span></li><li><span><a href=\"#5)-AUROC\" data-toc-modified-id=\"5)-AUROC-4.5\">5) AUROC</a></span></li><li><span><a href=\"#6)-F-bêta-score\" data-toc-modified-id=\"6)-F-bêta-score-4.6\">6) F-bêta score</a></span></li><li><span><a href=\"#7)-Confusion-matrix\" data-toc-modified-id=\"7)-Confusion-matrix-4.7\">7) Confusion matrix</a></span></li><li><span><a href=\"#8)-Job-score\" data-toc-modified-id=\"8)-Job-score-4.8\">8) Job score</a></span></li><li><span><a href=\"#9)-Table-to-store-all-models'-relevant-values-along-notebooks\" data-toc-modified-id=\"9)-Table-to-store-all-models'-relevant-values-along-notebooks-4.9\">9) Table to store all models' relevant values along notebooks</a></span></li><li><span><a href=\"#10)-SHAP\" data-toc-modified-id=\"10)-SHAP-4.10\">10) SHAP</a></span></li><li><span><a href=\"#11)-Any-python-object-serialization-to-string-and-deserialization\" data-toc-modified-id=\"11)-Any-python-object-serialization-to-string-and-deserialization-4.11\">11) Any python object serialization to string and deserialization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Importations and global settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Importation of required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### File management ###\n",
    "\n",
    "# Files' path.\n",
    "import os.path\n",
    "\n",
    "# Save and load files.\n",
    "import csv\n",
    "import pickle\n",
    "import base64 # Allow to seriliaze and deserialize any object as a string with pickle.\n",
    "\n",
    "\n",
    "### Data manipulations ###\n",
    "\n",
    "import numpy as np\n",
    "from numpy import set_printoptions # Saving full data when exporting to csv format.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "### Date & time ###\n",
    "\n",
    "# Time measurment and datetime management\n",
    "import datetime as dt\n",
    "from time import time\n",
    "\n",
    "\n",
    "### Warnings removal ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "### Data visualizations ###\n",
    "\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "### Additional common libraries ###\n",
    "\n",
    "from numpy import argmax, argmin\n",
    "import math\n",
    "from random import sample as py_rd_sp # Python random sampling.\n",
    "\n",
    "# Those allow to transform the shap values from their logodd format to odd.\n",
    "import copy\n",
    "from scipy.special import expit # Opposed of logit.\n",
    "\n",
    "\n",
    "### sklearn tools & libraries ###\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, precision_recall_curve, fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import make_scorer # Allow to make a sklearn custom scorer (For the custom job score).\n",
    "\n",
    "\n",
    "### Imbalanced data management ###\n",
    "\n",
    "from imblearn.pipeline import Pipeline # NB: imbalearn.pipeline.Pipeline allows to properly deal the SMOTE on the train set and avoid the validation/test sets.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTENC # NB: SMOTENC can manage categorial features while SMOTE cannot.\n",
    "\n",
    "\n",
    "### Features interpretability ###\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Settings of global graphics parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set default figure parameters for the whole notebook ###\n",
    "\n",
    "# Default parameters for matplotlib's figures.\n",
    "#plt.rcParams['figure.figsize'] = [6,6]\n",
    "#plt.rcParams['figure.dpi'] = 200\n",
    "#plt.rcParams['axes.spines.right'] = False\n",
    "#plt.rcParams['axes.spines.top'] = False\n",
    "#plt.rcParams['xtick.bottom'] = True\n",
    "#plt.rcParams['ytick.left'] = True\n",
    "\n",
    "# Default parameters of seaborn's figures.\n",
    "#sns.set_style('white') # NB: Needs to be above sns.set_theme to properly attend custom_params.\n",
    "#custom_params = {'axes.spines.right': False, 'axes.spines.top': False}\n",
    "#sns.set_theme(style='ticks', palette='deep', rc=custom_params) # All seaborn and matplolib figures will display with the seaborn's configurations.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Global files' path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global file paths.\n",
    "#EXPORTS_DIR_PATH = 'Exports'\n",
    "EXPORTS_MODELS_DIR_PATH = r'Exports\\Models\\Tried'\n",
    "IMPORTS_DIR_PATH = r'Exports\\Preprocessed_data'\n",
    "\n",
    "CSV_MODELS_FILE = 'models_info.csv'\n",
    "PKL_MODELS_FILE = 'models_info.pkl'\n",
    "#JSON_MODELS_FILE = 'models_info.json'\n",
    "#DATASETS_DIR_PATH = r'D:\\0Partage\\MP-P2PNet\\MP-Sync\\MP-Sync_Pro\\Info\\OC_DS\\Projet 7\\Datasets' #os.path.join('D:', '0Partage', 'MP-P2PNet', 'MP-Sync', 'MP-Sync_Pro', 'Info', 'OC_DS', 'Projet 7', 'Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_EZ (x, max_rows = 100, max_cols = 100, max_colwidth = 100):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Allows to display pandas dataframes with the number of rows and columns whished in an easy manner.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame()\n",
    "        Dataframe to display.\n",
    "    max_rows: int\n",
    "        Maximum number of rows to display.\n",
    "    max_cols: int\n",
    "        Maximum number of columns to display.\n",
    "    max_colwidth: int\n",
    "        Maximum width of each column.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    with pd.option_context('display.max_rows', max_rows, 'display.max_columns', max_cols, 'display.max_colwidth', max_colwidth):\n",
    "        display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_csv_full (df):\n",
    "    \n",
    "    \"\"\" \n",
    "    Long numpy.array() stored within a cell of the csv file are truncated and stringify\n",
    "    loosing most of the information and breaking the object.\n",
    "    This small function changes the numpy.array() displaying limit to truncate the object for its full exportation and\n",
    "    avoid data loss.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the numpy array number of items cutting threshold to a very high number and avoid the cut.\n",
    "    set_printoptions(threshold=1e100, linewidth=1e100)\n",
    "    \n",
    "    # Save the df to a csv file.\n",
    "    df.to_csv(os.path.join(EXPORTS_MODELS_DIR_PATH, CSV_MODELS_FILE))\n",
    "    \n",
    "    # Reset the numpy array number of items cutting threshold to default.\n",
    "    set_printoptions(threshold=100, linewidth=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Dataframes optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory(df):\n",
    "    \n",
    "    \"\"\" Reduce memory usage of a dataframe by setting data types. \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Initial df memory usage is {:.2f} MB for {} columns'\n",
    "          .format(start_mem, len(df.columns)))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            cmin = df[col].min()\n",
    "            cmax = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # Can use unsigned int here too\n",
    "                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    memory_reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print('Final memory usage is: {:.2f} MB - decreased by {:.1f}%'.format(end_mem, memory_reduction))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_int_cols (df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find and change the column dtype as int for columns with only int values.\n",
    "    This allows to spot all numerical categorical columns.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for col in df.columns:\n",
    "        for item in df[col]:\n",
    "            \n",
    "            # Test all items within the column to check if they all are integers.\n",
    "            if item == int(item):\n",
    "                if df[col].dtype != \"int64\":\n",
    "                    df[col] = df[col].astype(\"int64\")\n",
    "                    \n",
    "            # If only one of the items tested is not an int, the test is stopped and it switch to the next column.\n",
    "            else:\n",
    "                #if df[col].dtype != \"float64\":\n",
    "                #    df[col] = df[col].astype(\"float64\") \n",
    "                break\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model fitting and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_predict (model, X, y, cv, X_valid):\n",
    "    \n",
    "    \"\"\" Fit the model and get its predictions probabilities over a defined validation set or cross validation. \"\"\"\n",
    "    \n",
    "    # Fit and predict probabilities by cross validation (=> No need to fit it before and does not make much sense).\n",
    "    if cv != 0:\n",
    "        t0 = time()\n",
    "        yhat = cross_val_predict(model, X, y, cv=cv, method='predict_proba')\n",
    "        process_time = time() - t0\n",
    "\n",
    "    # Fit and predict (no cv).\n",
    "    else:\n",
    "        t0 = time()\n",
    "        model.fit(X, y)\n",
    "        yhat = model.predict_proba(X_valid)\n",
    "        process_time = time() - t0\n",
    "    \n",
    "    return yhat, process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_pred_list (y_pred_proba_P, l_proba_thrs = np.linspace(0, 1, num=100)):\n",
    "    \n",
    "    \"\"\" Classify all customers of the sample for each probability threshold. \"\"\"\n",
    "    \n",
    "    l_y_pred = []\n",
    "    for proba_thr in l_proba_thrs:\n",
    "\n",
    "        y_pred = []\n",
    "        for proba in y_pred_proba_P:\n",
    "            if proba > proba_thr:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "        \n",
    "        l_y_pred.append(y_pred)\n",
    "                \n",
    "    return l_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp_fp_fn_tn_lists (y_true, l_y_pred):\n",
    "\n",
    "    \"\"\" Get the confusion matrix values (binary classification). \"\"\"\n",
    "    \n",
    "    l_tp = []\n",
    "    l_fp = []\n",
    "    l_fn = []\n",
    "    l_tn = []\n",
    "    for y_pred in l_y_pred:\n",
    "        #cm = confusion_matrix(y_true, y_pred)\n",
    "        #l_tn.append(cm[0][0])\n",
    "        #l_fp.append(cm[0][1])\n",
    "        #l_fn.append(cm[1][0])\n",
    "        #l_tp.append(cm[1][1])\n",
    "        #display(y_true[y_true == 1])\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        l_tp.append(tp)\n",
    "        l_fp.append(fp)\n",
    "        l_fn.append(fn)\n",
    "        l_tn.append(tn)\n",
    "\n",
    "    \n",
    "    return l_tp, l_fp, l_fn, l_tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Optimization of the probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_proba_thr (np_tp, np_fp, np_fn, np_tn, l_proba_thrs, fn_cost_coeff = 10):     \n",
    "        \n",
    "    \"\"\" Find the best probability threshold for the passed data. \"\"\"\n",
    "        \n",
    "    ### Calculation of the best probability threshold.\n",
    "    \n",
    "    # Method 1 (Less accurate at low number of thresholds tried): Get the index of the closest FP/FN ratio of 1.\n",
    "    #J = np_fp / (fn_cost_coeff * np_fn)\n",
    "    #j_opt = 0\n",
    "    #for idx, j in enumerate(J):\n",
    "    #    if (j < 1 and j > j_opt) or (j > 1 and j < j_opt):\n",
    "    #        j_opt = j\n",
    "    #        J_opt_idx = idx\n",
    "    #print(\"Corresponding optimal ratio found:\", J[J_opt_idx])\n",
    "    \n",
    "    # Method 2 (More accurate at low number of thresholds tried):\n",
    "    # NB: As it had been seen within the global EDA notebook, there is 1 FN for 10 FP.\n",
    "    #     Our manager suggested that we should consider that 1 FN cost ~10 times more then 1 FP.\n",
    "    #     => No need to add a coefficient in front of FN to add more weight since it is already\n",
    "    #        taken into account within the classification balancement for the 2nd method.\n",
    "    #        => fn_cost_coeff = 10 --> 1\n",
    "    \n",
    "    tpr = np_tp / (np_tp + fn_cost_coeff/fn_cost_coeff * np_fn)\n",
    "    fpr = np_fp / (np_fp + np_tn)\n",
    "    J = tpr - fpr\n",
    "    J_opt_idx = argmax(J)\n",
    "    \n",
    "    # Method 3:\n",
    "    #np_fp_hypothesis = np_fp + fn_cost_coeff * np_fn\n",
    "    #J_opt_idx = argmin(np_fp_hypothesis)\n",
    "    \n",
    "    # Get the optimal probability threshold.\n",
    "    best_thr = l_proba_thrs[J_opt_idx]\n",
    "\n",
    "    # Print scores.\n",
    "    #print('Best Threshold: %.3f' % best_thr)\n",
    "    #print('Number of FP =', np_fp[J_opt_idx])\n",
    "    #print('Number of FN =', np_fn[J_opt_idx], '~ FP =', 10 * np_fn[J_opt_idx])\n",
    "    #print('Equivalent number of FP =', np_fp_hypothesis[J_opt_idx])\n",
    "    \n",
    "    return best_thr, J_opt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_density (y_true, y_pred_proba_P, best_thr):\n",
    "    \n",
    "    \"\"\" Find and show the optimal probability threshold on a figure. \"\"\"\n",
    "    \n",
    "    # Find the best threshold value graphically.\n",
    "    y_true_P = y_true[y_true == 1]\n",
    "    y_true_N = y_true[y_true == 0]\n",
    "\n",
    "    # Plot the probability density approximation of the TN.\n",
    "    #plt.hist(y_pred_proba_P[y_true_N.index], bins=100, density=True)\n",
    "    kde_N = sns.kdeplot(y_pred_proba_P[y_true_N.index], fill=True, alpha=0.5, edgecolor='k') #multiple=\"stack\"\n",
    "    \n",
    "    # Plot the probability density approximation of the FN.\n",
    "    #plt.hist(y_pred_proba_P[y_true_P.index], bins=100, density=True)\n",
    "    sns.kdeplot(y_pred_proba_P[y_true_P.index], fill=True, alpha=0.5, edgecolor='k')\n",
    "\n",
    "    # Plot a line at the best threshold found.\n",
    "    plt.vlines(best_thr, ymin=0, ymax=max(kde_N.get_yticks()), colors='k', linestyles='--')\n",
    "    \n",
    "    # Set other figures' parameters.\n",
    "    plt.title(\"Distribution of the probability a customer default\")\n",
    "    plt.xlabel(\"Probability thresholds\")\n",
    "    plt.legend([\"Regular customers\", \"Default customers\"])\n",
    "    \n",
    "    # Draw the figure.\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_sum_fp_coeff_fn (np_fp, np_fn, l_proba_thrs, best_thr, fn_cost_coeff = 10):\n",
    "    \n",
    "    \"\"\" \n",
    "    According to the coefficients set (FN = 10 x FP) calculate the theorical corresponding total number of\n",
    "    false predictions for each probability thresholds passed and draw the graph.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply the cost hypothesis and convert FN to its supposed corresponding number of FP.\n",
    "    np_fp_hypothesis = np_fp + fn_cost_coeff * np_fn\n",
    "    \n",
    "    # Plot the corresponding curve.\n",
    "    plt.plot(l_proba_thrs, np_fp_hypothesis)\n",
    "     \n",
    "    # Plot a line at the best threshold found.\n",
    "    plt.vlines(best_thr, ymin=0, ymax=max(np_fp_hypothesis), colors='k', linestyles='--')\n",
    "    \n",
    "    # Set other figures' parameters.\n",
    "    plt.title(\"Total number of corresponding FP according to probability thresholds\")\n",
    "    plt.xlabel(\"Probability thresholds\")\n",
    "    plt.ylabel(\"Total false positives\")\n",
    "    \n",
    "    # Draw the figure.\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_roc (y_true, l_yhats, l_model_labels):\n",
    "    \n",
    "    \"\"\" Draw the ROC graph and show its corresponding AUC value in the legend. \"\"\"\n",
    "    \n",
    "    idx = 0\n",
    "    for idx in range(len(l_model_labels)):\n",
    "        model_label = l_model_labels[idx]\n",
    "        yhat = l_yhats[idx]\n",
    "            \n",
    "        # Calculate inputs for the roc curve.\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, yhat)\n",
    "        \n",
    "        # Calculate the corresponding AUC.\n",
    "        auroc = roc_auc_score(y_true, yhat)\n",
    "    \n",
    "        # Plot the roc curves.\n",
    "        plt.plot(fpr, tpr, marker='.', markersize=2, label=model_label + \" (AUC = %.3f)\" % auroc)\n",
    "        \n",
    "        # Iterate the index value for the next loop.\n",
    "        idx += 1\n",
    "    \n",
    "    # Plot the no skill roc curve (the diagonal line).\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='No skill (AUC = 0.5)', color='k', alpha=0.75)\n",
    "    \n",
    "    # Set axis labels and the title.\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC\")\n",
    "    \n",
    "    # Show the legend.\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) F-bêta score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fbeta_score (l_proba_thrs, l_fbeta, beta, best_thr, best_thr_idx):\n",
    "    \n",
    "    \"\"\" Get the F-Bêta score for the best threshold found. \"\"\"\n",
    "    \n",
    "    # Get the optimal F-bêta score.\n",
    "    print('F-Bêta score of the optimal threshold found = %.3f' % l_fbeta[best_thr_idx])\n",
    "    print('Highest F-Bêta score = %.3f' % max(l_fbeta))\n",
    "    \n",
    "    return max(l_fbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_fbeta_score (l_proba_thrs, l_fbeta, best_thr):\n",
    "    \n",
    "    \"\"\" Draw the F-Bêta score figure for all probability thresholds tried. \"\"\"\n",
    "    \n",
    "    # Plot the graph.\n",
    "    plt.plot(l_proba_thrs, l_fbeta)\n",
    "    \n",
    "    # Plot a line at the best threshold found.\n",
    "    plt.vlines(best_thr, ymin=0, ymax=max(l_fbeta), colors='k', linestyles='--')\n",
    "     \n",
    "    # Set other figures' parameters.\n",
    "    plt.title('F-Bêta score = f(Probability thresholds)')\n",
    "    plt.xlabel('Probability thresholds')\n",
    "    plt.ylabel('F-Bêta score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_confusion_matrix (y_true, y_pred):\n",
    "    \n",
    "    \"\"\" Color the confusion matrix cases within a dataframe. \"\"\"\n",
    "    \n",
    "    def cm_df_2x2_style (serie):\n",
    "        \n",
    "        \"\"\" Set the style of the confusion matrix. \"\"\"\n",
    "        \n",
    "        cm_case = str(serie[0][0] + serie[0][1])\n",
    "        if cm_case == 'TN':\n",
    "            return ['background-color: #19C938', 'background-color: #FFC400'] # [TN, FP] | Sns deep green: #55A868, sns deep orange: #DD8453\n",
    "        elif cm_case == 'FN':\n",
    "            return ['background-color: #E8000B', 'background-color: #19C938'] # [FN, TP] | Sns deep red: #C44D52\n",
    "    \n",
    "    # Get the confusion matrix values.\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Create the dataframe which will stores and show the 4 values.\n",
    "    df = pd.DataFrame({'Negatives': [\"TN: \" + str(tn), \"FP: \" + str(fp)], 'Positives': [\"FN: \" + str(fn), \"TP: \" + str(tp)]},\n",
    "                      index=['Negatives', 'Positives'])\n",
    "    \n",
    "    # Set the dataframe style.\n",
    "    df = df.style.apply(cm_df_2x2_style)\n",
    "    \n",
    "    # Display the dataframe.\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Job score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_norm (y_true, y_pred, fn_coeff = -10, fp_coeff = -1, tp_coeff = 0, tn_coeff = 0):\n",
    "\n",
    "    \"\"\" Calculate the job score which will be the main score used to measure models performances. \"\"\"\n",
    "\n",
    "    # Get the confusion matrix coeffs.\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Gain.\n",
    "    g = tp_coeff*tp + tn_coeff*tn + fp_coeff*fp + fn_coeff*fn\n",
    "    \n",
    "    # Maximum gain.\n",
    "    g_max = tn_coeff*(fp + tn) + tp_coeff*(fn + tp)\n",
    "    # Minimum gain.\n",
    "    g_min = fp_coeff*(fp + tn) + fn_coeff*(fn + tp)\n",
    "    # Normalized gain (MinMax).\n",
    "    g_norm = (g - g_min) / (g_max - g_min)\n",
    "\n",
    "    return g_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_w_thr_fct (y_true, y_pred_proba_P, scorer = 'g_norm', verbose = True):\n",
    "    \n",
    "    \"\"\" Function which make the scorer take into account the best probability threshold found for the current set of hyperparameters. \"\"\"\n",
    "    \n",
    "    # Set the probability threshold range to try.\n",
    "    l_proba_thrs = np.linspace(0, 1, num=101)\n",
    "    \n",
    "    # Get all set of predictions for each threshold.\n",
    "    l_y_pred = get_y_pred_list(y_pred_proba_P)\n",
    "    \n",
    "    # Get the confusion matrix results for each set of predictions.\n",
    "    np_tp, np_fp, np_fn, np_tn = np.array(get_tp_fp_fn_tn_lists(y_true, l_y_pred))\n",
    "    \n",
    "    # Get the best threshold and its corresponding index with the list of the predictions sets.\n",
    "    best_thr, idx = opt_proba_thr(np_tp, np_fp, np_fn, np_tn, l_proba_thrs, fn_cost_coeff = 10)\n",
    "    best_thr = round(best_thr, 2)\n",
    "\n",
    "    # Select the prediction set corresponding to the best threshold found.\n",
    "    y_pred = l_y_pred[idx+1]\n",
    "\n",
    "    \n",
    "    # Select the scorer to use.\n",
    "    if scorer == 'g_norm':\n",
    "        # Calculate the normalized gain.\n",
    "        score = gain_norm(y_true, y_pred, fn_value=-10, fp_value=0, tp_value=0, tn_value=1)\n",
    "    elif scorer == 'auroc':\n",
    "        score = roc_auc_score(y_true, y_pred)\n",
    "    elif scorer == 'fbeta':\n",
    "        score = f_beta_score(y_true, y_pred, beta=3)\n",
    "        \n",
    "    \n",
    "    if verbose:\n",
    "        print('Best probability threshold:', best_thr)\n",
    "\n",
    "    return score, best_thr\n",
    "\n",
    "# Make the function as a new scorer for sklearn.\n",
    "g_norm_scorer_w_thr = make_scorer(scorer_w_thr_fct, scorer = 'g_norm', verbose=False)#, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_fct (y_true, y_pred_proba_P, scorer = 'g_norm', verbose = True):\n",
    "    \n",
    "    \"\"\" Function which make the scorer take into account the best probability threshold found for the current set of hyperparameters. \"\"\"\n",
    "    \n",
    "    # Set the probability threshold range to try.\n",
    "    l_proba_thrs = np.linspace(0, 1, num=101)\n",
    "    \n",
    "    # Get all set of predictions for each threshold.\n",
    "    l_y_pred = get_y_pred_list(y_pred_proba_P)\n",
    "    \n",
    "    # Get the confusion matrix results for each set of predictions.\n",
    "    np_tp, np_fp, np_fn, np_tn = np.array(get_tp_fp_fn_tn_lists(y_true, l_y_pred))\n",
    "    \n",
    "    # Get the best threshold and its corresponding index with the list of the predictions sets.\n",
    "    best_thr, idx = opt_proba_thr(np_tp, np_fp, np_fn, np_tn, l_proba_thrs, fn_cost_coeff = 10)\n",
    "    best_thr = round(best_thr, 2)\n",
    "\n",
    "    # Select the prediction set corresponding to the best threshold found.\n",
    "    y_pred = l_y_pred[idx+1]\n",
    "\n",
    "    \n",
    "    # Select the scorer to use.\n",
    "    if scorer == 'g_norm':\n",
    "        # Calculate the normalized gain.\n",
    "        score = gain_norm(y_true, y_pred, fn_value=-10, fp_value=0, tp_value=0, tn_value=1)\n",
    "    elif scorer == 'auroc':\n",
    "        score = roc_auc_score(y_true, y_pred)\n",
    "    elif scorer == 'fbeta':\n",
    "        score = f_beta_score(y_true, y_pred, beta=3)\n",
    "        \n",
    "    \n",
    "    if verbose:\n",
    "        print('Best probability threshold:', best_thr)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Make the function as a new scorer for sklearn.\n",
    "g_norm_scorer = make_scorer(scorer_fct, scorer = 'g_norm', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Table to store all models' relevant values along notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizing_table (df, l_vars, eval_dataset, l_col_labels):\n",
    "    \n",
    "    \"\"\" Update the table which sums up all models tried. \"\"\"\n",
    "    \n",
    "    ### Variables unpacking ###\n",
    "    # Labels.\n",
    "    model_label_key, model_key, \\\n",
    "    yhat_train_key, yhat_test_key, \\\n",
    "    best_thr_train_key, best_thr_test_key, \\\n",
    "    g_norm_train_key, g_norm_test_key, \\\n",
    "    rocauc_train_key, rocauc_test_key, \\\n",
    "    fbeta_train_key, fbeta_test_key, \\\n",
    "    process_time_train_key, process_time_test_key, \\\n",
    "    cm_vals_train_key, cm_vals_test_key = l_col_labels\n",
    "    \n",
    "    # Values.\n",
    "    model_label, model, yhat, best_thr, g_norm, rocauc, fbeta, process_time, cm_vals = l_vars\n",
    "\n",
    "    \n",
    "    ### Select if the values corresponds to the validation set or the test set ###\n",
    "    if eval_dataset == 'valid_set':\n",
    "        dict_val = {yhat_train_key: yhat,\n",
    "                    best_thr_train_key: best_thr,\n",
    "                    g_norm_train_key: g_norm,\n",
    "                    rocauc_train_key: rocauc,\n",
    "                    fbeta_train_key: fbeta,\n",
    "                    process_time_train_key: process_time,\n",
    "                    cm_vals_train_key: cm_vals\n",
    "                   }\n",
    "\n",
    "    else: # test_set\n",
    "        dict_val = {yhat_test_key: yhat,\n",
    "                    best_thr_test_key: best_thr,\n",
    "                    g_norm_test_key: g_norm,\n",
    "                    rocauc_test_key: rocauc,\n",
    "                    fbeta_test_key: fbeta,\n",
    "                    process_time_test_key: process_time,\n",
    "                    cm_vals_test_key: cm_vals\n",
    "                   }\n",
    "    \n",
    "    dict_model = {model_label_key: model_label, model_key: model}\n",
    "    \n",
    "\n",
    "    ### Create or update the right line in the dataframe ###\n",
    "    # NB: In python > 3.9 it is possible to merge dictionaries with \"|\" (the last dictionary takes the priority in conflicts).\n",
    "    \n",
    "    # Create a new row if it does not exist.\n",
    "    if model_label not in df.index:\n",
    "        print(\"Creating new entry...\")\n",
    "        df.loc[model_label] = dict_model | dict_val #[None] * df.shape[1]\n",
    "        print(\"Done!\")\n",
    "        \n",
    "    # Update the row.\n",
    "    else:\n",
    "        print(\"Updating entry...\")\n",
    "        df.loc[model_label] = df.loc[model_label].to_dict() | dict_model | dict_val \n",
    "        print(\"Done!\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sum_table (df, l_vars, get_csv_file, eval_dataset, main_scorer_val, l_col_labels = ['col0'],\n",
    "                      main_scorer_train_label = 'Job_score_train', main_scorer_test_label = 'Job_score_test',\n",
    "                      force_update = False):\n",
    "\n",
    "    \"\"\" Create or update and save a new table to sum up all models tried. \"\"\"\n",
    "    \n",
    "    # Update the csv file if the main score is higher.\n",
    "    if get_csv_file:\n",
    "        \n",
    "        # Reload the csv file in a df (created or already loaded at the beginning of the notebook).\n",
    "        df = pd.read_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))#.set_index('Model_labels')\n",
    "        \n",
    "        # Remove the initializing row (first row of filled with None) if one of the added rows are already full.    \n",
    "        if df.index[0] == None:\n",
    "            i = 0\n",
    "            for i in range(df.shape[0]):\n",
    "                if sum(df.iloc[i].isna()) == 0:\n",
    "                    df.dropna(how='all', inplace=True) # Drop the now useless first row.\n",
    "                    #df = df.convert_dtypes() # Infere all dtypes to according to the data type under each column.\n",
    "                    break\n",
    "        \n",
    "        # Check if the measures are from the test set or the train set.\n",
    "        if eval_dataset == 'valid_set':\n",
    "            main_scorer_label = main_scorer_train_label\n",
    "        else:\n",
    "            main_scorer_label = main_scorer_test_label\n",
    "        \n",
    "        # Check if the model_label entry is in the df.\n",
    "        model_label = l_vars[0]        \n",
    "\n",
    "        if model_label not in df.index:\n",
    "            df = summarizing_table(df, l_vars, eval_dataset, l_col_labels)\n",
    "            #df_to_csv_full(df)\n",
    "            df.to_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))\n",
    "            print('The new informations have been saved in a new row.')\n",
    "        \n",
    "        # Check if the score is inferior from the one already stored in the csv file.\n",
    "        elif df.loc[model_label, main_scorer_label] < main_scorer_val or pd.isnull(df.loc[model_label, main_scorer_label]) or force_update:\n",
    "            df = summarizing_table(df, l_vars, eval_dataset, l_col_labels)\n",
    "            #df_to_csv_full(df)\n",
    "            df.to_pickle(os.path.join(EXPORTS_MODELS_DIR_PATH, PKL_MODELS_FILE))\n",
    "            print('The row have been updated.')\n",
    "        \n",
    "        # Don't update if the new score is below the one in the csv file.\n",
    "        else:\n",
    "            print('The new score is inferior to the one already saved.')\n",
    "            print('Dataframe not saved.')\n",
    "\n",
    "            \n",
    "    # Do not update the csv file (set by user).\n",
    "    else: \n",
    "        df = summarizing_table(df, l_vars, eval_dataset, l_col_labels)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: [OBSOLET] because of the shap.TreeExplainer()'s parameter model_output='probability' which convert raw shap values as \n",
    "#     logodd directly to thier odd counter part.\n",
    "def logodd_to_odd (explanations, yhat, cat_class):\n",
    "    \n",
    "    \"\"\" Convert the logodd values to their odd counter parts. \"\"\"\n",
    "    \n",
    "    # Initialize the explanation objects which will store the transformed values.\n",
    "    explanation = copy.deepcopy(explanations)\n",
    "    explanations_transformed = copy.deepcopy(explanations)\n",
    "    \n",
    "    # Store the length of each client's shape values in logodd.\n",
    "    len_values = len(explanations[0])\n",
    "    \n",
    "    # Transform values.\n",
    "    for i in range(len(explanations.values)):\n",
    "\n",
    "        # Reformat the explanation attributes to their normal format.\n",
    "        explanation.values = explanations.values[i].reshape(1, len_values)\n",
    "                       \n",
    "        # Select the probability to untransform (Select the value corresponding to the selected category's class).\n",
    "        base_value = explanation.base_values[i]\n",
    "\n",
    "        # Compute the original_explanation_distance to construct the distance_coefficient later on.\n",
    "        original_explanation_distance = np.sum(explanation.values, axis=1)#[cat_class]\n",
    "\n",
    "        # Get the untransformed base value (Odd).\n",
    "        base_value_trf = 1 - expit(base_value) # = 1 - 1 / (1 + np.exp(-transformed_base_value))\n",
    "\n",
    "        # Compute the distance between the model_prediction and the untransformed base_value.\n",
    "        distance_to_explain = yhat[i][cat_class] - base_value_trf\n",
    "\n",
    "        # The distance_coefficient is the ratio between both distances cat_class will be used later on.\n",
    "        distance_coefficient = original_explanation_distance / distance_to_explain\n",
    "\n",
    "        # Transform the original shapley values to the new scale (Odd scale).\n",
    "        explanations_transformed.values[i] = explanation.values / distance_coefficient        \n",
    "        \n",
    "        # Finally reset the base_value as it does not need to be transformed.\n",
    "        explanations_transformed.base_values[i] = base_value_trf       \n",
    "        \n",
    "    # Return the transformed array from the logodd to the odd scale.\n",
    "    return explanations_transformed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretability_shap (model, scaler, X_train, X_test, cat_class = 0): #customer_idx = None\n",
    "      \n",
    "    \"\"\" Get the SHAP values in their odd form. \"\"\"\n",
    "        \n",
    "    # Scale the train and the test set to fit the model pipeline inputs format.\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert the numpy array X_test to a dataframe to associate the columns' labels to their corresponding values.\n",
    "    # NB: This is required for some shap graphic which cannot get the columns' labels otherwise.\n",
    "    X_test_norm = pd.DataFrame(X_test_norm, columns=X_test.columns)\n",
    "    \n",
    "    # Select data to interpret as global or local.\n",
    "    #if customer_idx == None: # Global.\n",
    "        #pd.DataFrame(X_test_norm, columns=X_test.columns)\n",
    "    #else: # Local.\n",
    "        #X_test_norm = pd.DataFrame(X_test_norm[customer_idx].reshape(1,-1), columns=X_test.columns)\n",
    "    \n",
    "    # Initialize time to measure the process duration.\n",
    "    t0 = time()\n",
    "\n",
    "    # Create the explainer model with TreeExplainer().\n",
    "    # NB: model_output='probability' allows to display shap values on the probability scale (odd).\n",
    "    explainer_shap = shap.TreeExplainer(model, X_train_norm, model_output='probability')\n",
    "\n",
    "    # Get explanations (values = shap values, base_values = SHAP expected average values after its fit on X_train_norm, data = original data passed => X_test_norm).\n",
    "    explanations = explainer_shap(X_test_norm)\n",
    "    \n",
    "    # If the negative class is chosen so the SHAP's values gotten for the positive class (1) needs to be adapated.\n",
    "    if cat_class == 0:\n",
    "        explanations.values = - explanations.values\n",
    "        explanations.base_values = 1 - explanations.base_values\n",
    "\n",
    "    # Measure the process time duration.\n",
    "    delta_t = time() - t0\n",
    "\n",
    "    return explanations, delta_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Any python object serialization to string and deserialization\n",
    "\n",
    "Very useful to transfer any python object in json format across APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_to_txt (obj):\n",
    "    \n",
    "    \"\"\" Serialize an object into a plain text. \"\"\"\n",
    "    \n",
    "    message_bytes = pickle.dumps(obj)\n",
    "    base64_bytes = base64.b64encode(message_bytes)\n",
    "    txt = base64_bytes.decode('ascii')\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_obj (txt):\n",
    "    \n",
    "    \"\"\" De-serialize an object from its plain text serialization counter part. \"\"\"\n",
    "    \n",
    "    base64_bytes = txt.encode('ascii')\n",
    "    message_bytes = base64.b64decode(base64_bytes)\n",
    "    obj = pickle.loads(message_bytes)\n",
    "    \n",
    "    return obj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "544px",
    "width": "676px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "347.038px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
